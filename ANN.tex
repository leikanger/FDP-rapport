
%Så kjem seinare ANN. Kan lese mykje fra rapport i NEVR3004.


%
% TODO Skriv om oppgave 1: "Legg spesiell vekt på forhold som kan knyttes til stabilitet under læring og/eller tilbakekobling("recurrent ANN")." LEGG VEKT PÅ DETTE! TODO TODONOW!
% TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO 
%





%
%
% 		Disposisjon for kapittelet:
% 	 INNLEDNING til kapittelet:
% 	TODO Skrive at oppgaveteksten ber meg om å finne eksisterende modeller som representerer BÅDE fyringsrate og -frekvens. Dette finnes ikkje.
% 	Tenkte difor å heller gå gjennom historien for ANN, for at leser skal ha eit innblikk i kva som eksisterer av frekvens-ANN og etterkvart fyringstid-ANN.
% 	Skriv også "Why ANN?" i innledninga til kapittelet.
% 			- Om at Ann gir assossiative egenskaper til den algoritmiske computeren.
% 			- Om læring. I enkleste form: 
% 							- ulineært filter med gode læreegenskaper.
% 							- Natively distributed and asynchrounous. 
% 			- Anna ? 							
% 	Skriv "Why ANN?" først, så skriv det om oppgaveketsten, der eg slutter med å innlede neste section: "ANN history".
%
% 	... ... ... ... ... ... ... ... ... ... ... ... ... 
%
% 		\section{ Third generation ANNs }
% 			- Oppdagelsen av STDP satte fokus på relativ spike timing for pre- og post- synapsisk neuron.
% 			- Skrive om SANN. Bedre simulering av nodene (neuron). Sender ikkje signal vidare kvar tidsiterasjon, men bare når depol er over terskel. Basert på modell presented in Hodkin/Huxley, 1952 artikkel.
% 				"Thus, one of the fundamental questions of neuroscience is to determine if neurons communicated by a rate code or by a pulse code.[1]" 
% 					[1]: (Wulfram Gerstner (2001). "Spiking Neurons". In Wolfgang Maass and Christopher M. Bishop. Pulsed Neural Networks. MIT Press. ISBN 0262632217.)
% 			 	- Dette kan eg skrive masse om i forhold til modelle min: Aktivitet er koda med aktivitet ("frekvens"), mens syn.plast. er avhengig av timing på spikes. (?)
% 			- Pga. meir realistiske egenskaper og bedre simulering av neuronet, kan SANN også brukes for simuleringer til neuroscience.
% 				("They have proved useful in neuroscience, but (not yet) in engineering. " (fra wiki))
% 				 "To date, there have been no large scale spiking neural networks that solve computational tasks of the order and complexity of rate coded (second generation) neural networks." (Fra linja under på wiki)
% 			
% 			- SANN 	-  Korleis er dette for SANN?  Skriv at dette er en simulering av enkeltnodene i neurale nett, og at dette skjer i presens (input fører til auka verdi, over terskel fører til output, osv)
% 				- Skjer i presens, planlegger ikkje lenger enn kva som skal gjøres neste tidssteg (kø). Moore automata (basert bare på tilstand for enkeltnodene (depol.verdi)).
% 			- KANN  -  Korleis er dette for KANN?  Skriv at dette er en høgare ordens simulering av ligningene som er resultat av prosessane i enkeltnodene i neurale nett.
% 				- Dette skjer i presens, men planlegger også framtida: er framsynt. Mealy automata (basert på state + inputs).
% 			- Kanskje: Skriv om forskjellane mellom Moore og Mealy automata.
% 		\section{ KANN. }
% 			. Skrive at: dersom målet er å lage eit pragmatisk nyttig ANN med tidspunkt-info for spikes, bør SANN skrives helt om. Skriv om at SANN bruker mykje ressurser på å opprettholde en stabil tilstand i nettet.
% 			- Siden beregning kreves bare når verdiane endres (i biologiske NN), bør dette være fokus for ANN. SANN bruker mye ressurser på å opprettholde en stabil tilstand (dersom det ikkje er endring i nettet).
% 			- Det beste hadde vore å utvikla eit ANN som bare krever ressurser ved endring av input til nodene. Skrive litt fram og tilbake om dette. Ende opp med at vi trenger en Mealey automata? (dersom dette stemmer)
% 			-
% 			
% 			
% 			
% 			
% 			

\section{Artificial neural networks: background}
\label{secANNhistory}
%TODO Endre neste setning. Dårlig:
To illuminate different aspects of artificial neural networks (ANN) a short history about ANN is presented in sec. \ref{ssecHistoryOfANN}. 
The models presented in the following section are either of a historical interrest or of a more immediate relevance to this project.
Because this report compares two ANNs with information about the timing of the output transmission for a node, there will be a special focus on models with this capability in this introduction.  %XXX Stemmer dette?
%Because the new model and the model behind ``Spiking Artificial Neural Network'' will be compared in this project, we will have a special focus on SANN in the following review of ANN history.



Before we review the different ANN models in the history of ANN, we need to know a minuscule of the biological neuron. 
For a more comprehensive introduction it is referred to section \ref{secTheBiologicalNeuralSystem}.
Only the most important findings will be repeated here.

The biological neural system is comprised of nodes, the neuron, and connections between the nodes, the synapse.
%A neuron has a state that gives the value of the neuron. 
A neuron has a state, given by e.g. the electrical potential and the ionic consentration gradients for the different ions over the membrane.
For a simplified model of the neuron the value often refers to the electrical potential, or the ``depolarization'' of the neuron. % ELLER Bare "The value often refers to .." (uten det om simplified vertion..
In this text, ``the value'' refers to the depolarization whenever we speak about the biological neuron.

%The ``output'' of a neuron in the following paragraph refers to an action potential of the neuron.
If the neuron ``fires an action potential'', all the neuron's output synapses will transmit after a small delay.
The size of this transmission is given by the strength of the synaptic connection between the nodes, not the value of the neuron.
% KAffane ksal eg skrive i starten her? (under) 	xxx xxx
This size of the connection is often referred to as the ``synaptic weight'' of the synapse.

The value of the neuron is not unrelated to the synaptic transmission.
In the simplified neuron, the size of transmission is uncorrelated with the value of the node, but the transmission itself is given as a direct result of the value going over the ``firing threshold'' of the neuron.
% % The value of the neuron does not give the size of transmission for it's output synapses. It does however give the time of transmission.
%An action potential is only initialized when the value of the neuron goes to suprathreshold levels.
When the value reaches the firing threshold, an action potential is initialized.
The action potential causes a transmission to occur at all the neuron's output synapses.
%This causes a transmission through all the neuron's output synapses.
The synapses can be excitatory or inhibitory, causing an increase of decrease of the postsynaptic neuron's value. %\cite{PrinciplesOfNeuralScience4edKAP02}. % TODO Ta vekk alle referanser fra dette avsnittet? (Det er bare en..)

More information about the mechanisms of the neural system can be found in sec. \ref{secTheBiologicalNeuralSystem}. 
Relevant background information about the neuron and synaptic transmission is presented in section \ref{ssecTheNeuron} and section \ref{ssecTheSynapse}. 
The action potential is introduced in section \ref{ssecTheActionPotential}.
The curious reader may also find citations and references for further readings in these sections.

%TODO Skriv en overgang. Fra det over, til ANN-historie.

\subsection{Review of the History of ANN}
\label{ssecHistoryOfANN}
The pragmatic use of simulations of neural networks started with the McCulloch--Pitts neuron in 1943. 
Warren McCulloch, an early neuroscientist, and matematician Walter Pitts first began a formalized discussion of the use of the mechanisms of the neuron in technology. % Bra? eller dårlig: slutten av linja.
The result was the first artificial neural network, consisting of logical gates. McCullock and Pitts modelled the neuron as boolean devises (off--on response)\cite{MccullochPittsHistorie}. 
What is later referred to as the first generation ANN is based on the McCulloch--Pitts neuron\cite{Maass97networksof}.
One of these models is Rosenblatt's Perceptron\cite{HaykinANNbok}.

The next generation of ANNs have computational units with a continuous activation function. 
A common activation function is the \emph{sigmoid function} $\sigma(y)=\frac{1}{1+e^{-y}}$ \cite{HaykinANNbok}.
In sec. \ref{ssecTheNeuron} and \ref{ssecTheActionPotential}, the computational basis of networks of neurons will be introduced. 
We will see that the neuron has a boolean carracteristic, in the form that the neuron as a whole either sends output through all its output synapses or rests. 
%
%Neural nets of the second generation can therefore be viewed as ``firing rate interpretations'' of biological neural networks\cite{Maass97networksof}.
ANN of the second generation is often referred to as ``firing rate interpretations'' of biological neural networks\cite{Maass97networksof}.
%Over : få vekk "therefore". sikt heller på det cite-Maass seier.   UNDER: ".. will then be .." : dårlig. Gjør om!
%A simulation of a biological neural network by a 2. generation ANN will then be in the frequency domain, and have no information about the timing of the spikes. %TODO Sjekk om "spike" er innført tidligere.
A Simulation of biological neural network by a 2. generation ANN will then be in the frequency domain, and have no information about the timing of the node's spikes.
%In this case, the second generation ANN could be said to give a better simulation of neural systems than the first generation ANN.  %KVIFOR? XXX Er mevmt seinare..

% 2.gen er bedre enn første, fordi begge er "state less". For tidsdomenet blir dette heilt feil. For frekvensdomenet blir det mindre feil. (Heilt rett dersom du ser bort fra syn.p. og modulatory neurotransmitters.


%
% TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO 
% Sjå gjennom. Referer alle utsagn som ikkje eg har funnet!
%

Recently, the third generation of ANN has emerged. In this model, each node is simulated with respect to aspects thought important for signal processing\cite{Maass97networksof}.
The most used model for a spiking neuron is the ``Leaky--Integrate and Fire''(LIF) neuron\cite{florian03}. 

%todo: sjekk: skrevet for tidlig på morgenen.
In the LIF neuron model, the node's value is computed as a leaky integration of its input. 
All the input to the node is integrated over time with a leakage propotional to the value of the node, every time step.
The node's value is thus a result of the present excitatory and inhibitory input as well as the immediate history for the node.

%skrive først kva som gjøres. LIF
%så skrive at dette kalles for SANN.
When a node is excited above the firing threshold, an action potential or ``spike'' is generated in the node. 
When this spike reaches a synapse, after a simulated delay we get synaptic transmission at that synapse.
From the information presented in sec. \ref{secTheBiologicalNeuralSystem}, we can see that this is much closer to the biological system that previous ANN generations.
It is actually so close that the word ``simulation'' will occationally be used in this report.  	% ".. actually so closa that .." DÅRLIG. Fiks?
%We can see that this is a better simulation of neural systems than that of the first two generations of ANN, based on the information presented in section \ref{secTheBiologicalNeuralSystem}.

In the third generation ANN it is the action potentials or the "spikes", that is responsible for information processing.  %TODO Skriv om slutten / Feil ord.. 		".. or the "spikes" that is responsible for the information flow.
This ANN model is therefore often referred to as ``Spiking Artificial Neural Network''(SANN).
%TODO skrive at siden det er en simulering av neurale sys., er det kanskje eit poeng å først beskrive det originale systemet.
% 		Forbered leser / gjør leser nøgd med at eg har ei kapittel om det bioloiske systemet.


% TODO Skriv litt om bakgrunnen for at man starta med SANN:
% 		- Temporal coding : sjå "networks of spiking neurons: the third generation of neural network models"
% 		- STDP. Skriv minimalt om dette (forbered leser ved å såvidt nevne det). Dette skal skrives heir om i SANN-section.












% Skrive at forskjellen mellom de ulike generasjonene er på noden: kva gjør noden? Kva er input, activation function og output?

%TODO TODO TODO Ikkje summarize! Eg innfører nye evalueringer! 1. og 2. gen. ANN er stateless. 3. gen. har state!
To summarize the ANN history section,  we have three ``generations'' of artificial neural networks, each with a different model for the computational unit. %TODO Ikkje "its own set of computational units." Dette er feil..
What propagates through the network, the activation function and synaptic plasticity differs between them. 


The first generation of artificial neurons were so--called threshold gates, with a boolean output that was [true] if the summed input were above some threshold.
%with output of zero unless the input summed to some value above threshold, causing the output to become one.
Nodes of the second generation gave, in some respects, a better simulation of the biological neuron. % than the first.
%It does not have discrete ``states'' for the input, but have a continuous ``activation function'' giving output as a function of the node's immediate input.
The model does not classify the input to discrete ``states'', giving the output of the node. 
Instead in has a continuous ``activation function'' that gives output as a function of the node's immediate input.
%
This activation function is found to give the best result if the function is a continuous, differentiatable sigmoid funtion\ref{HaykinANNbok}.
%In the second generation ANN, the spatially summed input (summed over all the input synapses) is not classified between two states, but gives an output based on the continuous nonlinear activation function.
% TODO TODO TODO ODO TODO TODO TODO Sjå over neste linje. Gidd ikkje no..
If the transmissions between nodes is viewed as the firing frequency of the neuron, we can say that the continuous output value represents the output frequency as a function of the input frequency over the time step in the simulation.
																									%the firing frequency of the node is a function of the spatially and temporally summed input input to the node, 
																										%or the sum of the transmissions in all the node's input synapses.
In this case, the second generation ANN is closer to the biological neural network, than the ``threshold gates'' of the first generation. % ANN. 
% Eller 								is a better simulation of a biological neural system than the ``threshold gates'' of the first generation.
%Kanskje heller skrive at:             
%One could state that this is closer to the biological neuron, in respect to the biological neurons output frequency being a function of the frequency of the node's input.

%TODO skriv at 3.gen har state, og (det som står etter komma i neste linje).
The nodes of the third generation ANN became even more similar to the biological neuron, as the output of a node depend solely on the state of the node.
In the simplified neuron, the ``state'' of the neuron is represented by the neurons depolarization.
%In biology, the ``state'' of the neuron is represented by the neurons depolarization.
When this value reaches the threshold, we get an action potential in the neuron.
%This is the same principles that is the foundation of the nodes of SANN. 
In SANN, this is simulated by integrating all the node's input and firing a spike if this value reaches the firing threshold\cite{Kunkle02pulsedneural}.
Spiking Nodes (SN) can thus be said to be a direct simulation of the biological neuron.
%If we view this as the output value of the neuron as a whole, we can say that the output of a neuron is given by the temporally summed input to the neuron.
%The output of the neuron (an action potential in the neuron) will further activate synaptic transmission in each of the node's output synapses, causing a increase or decrease of the postsynaptic node's value.
The mechanisms of the biological neuron, the action potential and synaptic transmission will be reviewed in chapter \ref{secTheBiologicalNeuralSystem}.
%The action potential and synaptic transmission for the biological system is reviewed in section \ref{secTheBiologicalNeuralSystem}.


%TODO Skriv kvifor det er bedre med stateless noder for frekvensdomenet enn for noder i tidsdomenet.









%Over skal stå om: Kva er forskjellene mellom ulike modellers noder? Kva blir propagert gjennom nettet? Kva er aktivitets-funksjonen?

% ref_asdf2143@ANN
%TODO : Når eg begynner å skrive om KANN: Kan skrive om at eg innfører at output til kvar node igjen varierer som en funksjon av input. Tidspunkt for fyring er gitt av nodens state, og input vektoren.
% 	Dersom eg gir output kvar gang nodens input endres, blir output gitt av nodens state+input. Dette gir en Mealy automata av state maschine til enkeltnodane.
% 		Kan skrive at man kan også lett kjøre output til en node av de tidligere generasjonane ANN, ved å transformere aktivitetsvariabelen Kappa over til frekvens eller estimert fyringstid. (FETT!)






















% TODO : OK, no skriv litt om synaptisk plasticity. Fokus på stabilitet.


%\subsection{Synaptic Plasticity for artificital neural networks}
%TODO Skriv om synaptisk plastisitet (sjå oppgave 1 (fokuser spesiellt på forhold som kan knyttes til stabilitet under læring og/eller tilbakekobling("recurrent ANN").)
%Simple variants of ``Hebbian learning'' only give an increase in the synaptic weight, and is clearly exponentially unstable. Different variations of the naive model of synaptic 
%Because Hebb's postulate only considered poitive weight change, STDP has later been %omtalt som stable hebbian learning.
%As we will see later, a simple formulation based on ``Hebbian learning'' is unstable, because it only states a positive weight change. % As this varies with the activation  XXX Skriv at det er exponentially-ustabilt.
%and many variations has been devised to stabilize synaptic plasticity, many of them biologically unplausible.


%OGSÅ: NEVN: STDP.
% %In neuroscience, learning is |||xxx Skriv at synaptisk plasticity er samme som læring. (innføring til det neste, og mulighet til å innføre begrepet "synaptisk weight".
%In 1949 Donal O. Hebb proposed a famous postulate, later known as Hebb's postulate:
%\begin{quote}
%When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process og metabolic change takes place in one or both cells such that A's efficiency, as one of the cells firing B, is increased.\cite{Hebb1949Kap4}
%\end{quote}
%
%This postulate were formalized into formulas for synaptic plasticity, and has been extencively used as a learning rule for artificial neural networks.
%This has been referred to as ``Hebbian learning''\cite{HaykinANNbok}.
%%Hebb's postulate were later formalized into learning formulas for synaptic plasticity. 
%%``Hebbian learning'' is still used in some forms for neural networks. % TODO Skriv HEILT spesifikkt / finn referanser.


% %Heilt på slutten av Syn. P.: få overgang til neste kapittel: SANN :		Blandt andre ting, After the discovery of STDP, the timing of the action potential recieved an increased focus also in ANN models. 






%fra disposisjonene på toppen:
% 		\section{ Third generation ANNs }
% 			- Oppdagelsen av STDP satte fokus på relativ spike timing for pre- og post- synapsisk neuron.
% 			- Skrive om SANN. Bedre simulering av nodene (neuron). Sender ikkje signal vidare kvar tidsiterasjon, men bare når depol er over terskel. Basert på modell presented in Hodkin/Huxley, 1952 artikkel.
% 				"Thus, one of the fundamental questions of neuroscience is to determine if neurons communicated by a rate code or by a pulse code.[1]" 
% 					[1]: (Wulfram Gerstner (2001). "Spiking Neurons". In Wolfgang Maass and Christopher M. Bishop. Pulsed Neural Networks. MIT Press. ISBN 0262632217.)
% 			 	- Dette kan eg skrive masse om i forhold til modelle min: Aktivitet er koda med aktivitet ("frekvens"), mens syn.plast. er avhengig av timing på spikes. (?)
% 			- Pga. meir realistiske egenskaper og bedre simulering av neuronet, kan SANN også brukes for simuleringer til neuroscience.
% 				("They have proved useful in neuroscience, but (not yet) in engineering. " (fra wiki))
% 				 "To date, there have been no large scale spiking neural networks that solve computational tasks of the order and complexity of rate coded (second generation) neural networks." (Fra linja under på wiki)
% 			
% 			- SANN 	-  Korleis er dette for SANN?  Skriv at dette er en simulering av enkeltnodene i neurale nett, og at dette skjer i presens (input fører til auka verdi, over terskel fører til output, osv)
% 				- Skjer i presens, planlegger ikkje lenger enn kva som skal gjøres neste tidssteg (kø). Moore automata (basert bare på tilstand for enkeltnodene (depol.verdi)).
% 			- KANN  -  Korleis er dette for KANN?  Skriv at dette er en høgare ordens simulering av ligningene som er resultat av prosessane i enkeltnodene i neurale nett.
% 				- Dette skjer i presens, men planlegger også framtida: er framsynt. Mealy automata (basert på state + inputs).
% 			- Kanskje: Skriv om forskjellane mellom Moore og Mealy automata.
% 		\section{ KANN. }
% 			. Skrive at: dersom målet er å lage eit pragmatisk nyttig ANN med tidspunkt-info for spikes, bør SANN skrives helt om. Skriv om at SANN bruker mykje ressurser på å opprettholde en stabil tilstand i nettet.
% 			- Siden beregning kreves bare når verdiane endres (i biologiske NN), bør dette være fokus for ANN. SANN bruker mye ressurser på å opprettholde en stabil tilstand (dersom det ikkje er endring i nettet).
% 			- Det beste hadde vore å utvikla eit ANN som bare krever ressurser ved endring av input til nodene. Skrive litt fram og tilbake om dette. Ende opp med at vi trenger en Mealey automata? (dersom dette stemmer)
% 			-






	\subsection{Synaptic plasticity for the second generation ANN} % Eller "Hebbian learning"? Eller det over.
	From the previous discussion, we know that nodes of the second generation ANN only react to the present input to the node.
	% It does not contain a state, and have no way of computing the t NEI, har ikkje introdusert STDP enda..
	The nodes can therefore be seen as a nonlinear filter, calculating its output based on the summed input to the node.

	The synaptic plasticity can therefore only be calculated by comparing the output of the pre-- and postsynaptic node.
	%The discussion about the real  Hebbs original 
	%Teit å sjå på postsyn. sin output!
	Because this is the only comparable variable for the two nodes, learning rules comparing the output of the nodes are used. % in second generation ANN.
% 	%
	In the second generation ANN, local learning rules are often referred to as ``Hebbian learning''.

	In 1949 Donal O. Hebb proposed the famous Hebb's postulate:
 	\begin{quote}
	When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, 
		some growth process og metabolic change takes place in one or both cells such that A's efficiency, as one of the cells firing B, is increased.\cite{Hebb1949Kap4}
	\end{quote}

	This has been an important postulate for neuroscience as well as ANN.
	In ANN models, local learning rules has recived the name ``Hebbian learning''.

	\begin{equation}
		\delta w_{ij} = \sum{k r_i r_j'}
	\end{equation}
 	Where $w_{ij}$ is the synaptic weight between neuron j and i. 
	$r_i$ is the rate of neuron i and $r_j'$ is the output of neuron j. 
	The constant \mbox{k} is the learning constant, giving the level of synaptic plasticity. 
	%See \cite{Hebb1949Kap4}, where Hebb originally postulated Hebb's principle.

	%How close this lerning rule comes to Hebb's postulate will not be discussed in this text.
	Because the frequency is a defined as a positive size, the weight change $\delta w_{ij} = \sum{k r_i r_j'}$ will allways be either positive or negative depending on $k$. 
%Since the frequency is a defined as a positive size, the weight change $\delta w_{ij} = \sum{k r_i r_j'}$ will allways be either positive or negative depending on $k$. 
	If $k$ is a positive constant, $\delta w_{ij}$ is allways either positive or zero.
	As an increased synaptic weight $w_{ij}$ creates a stronger connection between node $j$ and node $i$, the product $k r_i r_j'$ will become correspondingly larger.
	This product gives the derived of the synaptic weight.
	We have positive feedback, and exponential unstability for the synaptic weight.
	For a megative $k$ we get negative feedback, and an exponentially stable system that will eventually get a synaptic weight of $w_{ij}=0$.

	Many atempts have been made to stabilize the synaptic growth for neural networks, e.g. \cite{hebbUstabilt}. %ikkje skriv "eg.", sjå heller om påstanden er skrevet i artikkelen og referer isåfall dette (da kan eg fjærne "eg.")
	The equations become increasingly complex, and is not biologically plausible.

% figur funker bare for pdflatex. Ta med til leveringa.
% TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO 
%\begin{figure}[htb!p]
%	\centering
%	\includegraphics[width=0.8\textwidth]{figurSTDP.jpeg}
%	\caption{Spike timing-dependent plasticity. a, Synapses are potentiated if the synaptic event precedes the postsynaptic spike. Synapses are depressed if the synaptic event follows the postsynaptic spike. b, The time window for synaptic modification. The relative amount of synaptic change is plotted versus the time difference between synaptic event and the postsynaptic spike. The amount of change falls off exponentially as the time difference increases. In addition, the amount of potentiation decreases for stronger synapses, whereas the relative amount of depression is independent of synaptic size.}
%	\cite{stableHebbVedSTDP}
%\end{figure}

%\subsection{SANN -- ``the third generation ANN''}
\subsection{Spiking Artificial Neural Network}
% 	"Thus, one of the fundamental questions of neuroscience is to determine if neurons communicated by a rate code or by a pulse code.[1]" 
% 	[1]: (Wulfram Gerstner (2001). "Spiking Neurons". In Wolfgang Maass and Christopher M. Bishop. Pulsed Neural Networks. MIT Press. ISBN 0262632217.)
The importance of the relative spike timing of the presynaptic vs. the postsynaptic neuron for synaptic plasticity led to development of Spiking Artificial Neural Network (SANN). 

SANN is a simulation of a network of neurons in the time domain. Activity of each node is represented as the neurons depolarization. 
In a simulation of neuron it is important to calculate the leakage of value each iteration, or in other ways so that the value is updated before it is used. 
%For this simulation of biological neurons, it is important to calculate the leakage of value each iteration, or in other ways so that the value is updated before it is used. 
%The behaviour of the neuron in the frequency domain follows that of the biological neuron since each node is a simulation of the biological neuron. %For this reason network aspects of the ANN will also aproximate that of biological NN
Simulating each neuron will be computationally demanding, as leakage, every spike and every transmission has to be simulated. %Dårlig setning..

SANN use boolean signals intracellularly. 
At for the biological synapse, the tranmission is given by the synaptic weigth. 

The major advantage of SANN is that it retains imformation about the timing of the spiking for the neurons.  %TODO Ikkje skriv "retains". Finn på noko som gir bedre flyt i teksten.
An important mechanism for synaptic plasticity is Spike-Time Dependent Plasticity (STDP).

Gustafsson et al. proposed in 1987 that the synaptic weight gain varied with the postsynaptic neurons depolarization after synaptic transmission\cite{Gustafsson03011987}. 
\begin{quote}
Moreover, the finding that homosynaptic tetanization produced little LTP after this pairing procedure suggests that LTP, at least over the time span examined, is controlled by postsynaptic depolarization and does not depend on high-frequency presynaptic activity for induction.\cite{Gustafsson03011987}.
\end{quote}
This has later been known as Spike Timing Dependent Plasticity (STDP)\cite{reviewSTDP}. 
The biophysiological background for STDP is introduced in appendix \ref{appendixSynPlast}.

STDP could be said to be a more elaborate version of Hebbian learning (synaptic plasticity). 
In STDP, it is not the activation level of the two nodes that is compared, but the timing of the transmission.
If the presynaptic node contributes immediately before the postsynaptic node fires, a strong increase in the synaptic weigth will be the resulting synaptic plasticity.
A contribution that ``comes to late'' will decrease the synaptic weigth instead of increasing it\cite{reviewSTDP}.


% TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO 
% 				TA MED FØR LEVERING :
%\begin{figure}[!htbp]
%        \centering
%        \includegraphics[width=0.8\textwidth]{figurSTDP.jpeg}
%        \caption{Spike Time Dependent Plasticity(STDP): The timing of the postsynaptic spike in relation to the presynaptic neurons contribution gives the size and sign of the change of synaptic weight\cite{reviewSTDP}.} 
%\end{figure}
% TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO 

The possibility for a decrease in synaptic weight after transmission in combination with other aspects, makes it possible to design an ``intrinically stable STDP learning rule''\cite{StableHebbianLearningFromSTDP}.
%LTD decreases the synaptic weight, and in combination with other aspects, makes it possible to design an ``intrinically stable STDP learning rule''\cite{StableHebbianLearningFromSTDP}.
% 	%
SANN is also computationally more powerful than the other generations ANN\cite{florian03}.
Because of the advantages of SANN, the model has become popular in the field of ANN, and is frequency referred to as the ``third generations ANN''.
%Because of the advantage following STDP, SANN is frequently referred to as ``third generation ANN''. 







%TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO 
%TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO 
%TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO 
% Fullfør, dersom tid. Ellers: kutt ut section. MEN DET ER VIKTIG Å HA MOTIVASJON: 			(evt. skriv motivasjon i innledning..)
%TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO 

% %Skriv tilslutt at simulering av kvart neuron virker lite effektivt. Det leda meg inn på tanken om KANN. Så skriv om KANN.
%\subsection{The motivation for developing a new model -- $\kappa$ANN}
% %\section{$\kappa$ANN --- The next generation?} %DETTE blri for mykje! ikkje gjør det eksplisitt! (la det ligge og ulme i leserens hode!
%As a computational system, a neural system is fundamentally different from the processor in a computer. 
%The neural system is based on a vast amount of computationally weak, massively parralell indivitual ``processors'' ---the neuron. 
%The computational unit of a computer, the processor, is funtamentally different. It is one or a few, computationally strong, serial processor(s). 
%
%In neural systems the neurons are directly connected, such that the output of one node is the input of another. In computers, the result of a computation is saved somewhere in memory and that can be accessed by (one of) the processor(s).
%
%With this fundamental difference betwee  \emph{XXX} SKRIV MEIR HER.






















