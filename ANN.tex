
%Så kjem seinare ANN. Kan lese mykje fra rapport i NEVR3004.



%
%
% 		Disposisjon for kapittelet:
% 	 INNLEDNING til kapittelet:
% 	TODO Skrive at oppgaveteksten ber meg om å finne eksisterende modeller som representerer BÅDE fyringsrate og -frekvens. Dette finnes ikkje.
% 	Tenkte difor å heller gå gjennom historien for ANN, for at leser skal ha eit innblikk i kva som eksisterer av frekvens-ANN og etterkvart fyringstid-ANN.
% 	Skriv også "Why ANN?" i innledninga til kapittelet.
% 			- Om at Ann gir assossiative egenskaper til den algoritmiske computeren.
% 			- Om læring. I enkleste form: 
% 							- ulineært filter med gode læreegenskaper.
% 							- Natively distributed and asynchrounous. 
% 			- Anna ? 							
% 	Skriv "Why ANN?" først, så skriv det om oppgaveketsten, der eg slutter med å innlede neste section: "ANN history".
%
% 		\section{ Background: ANN history }
% 			- Skrive om starten for ANN; Skriv kort om:
% 				- McCullock-Pitts neuron, 1943. 
% 					- Lineær funksjon med threshold. 
% 					- Boolske signal (ikkje syaptisk weight, syn.plast. (?) )
% 					- Feedforward.
% 				- Perceptron, 1957 ("simplest form of feedforward ANN" - wiki) 		Sjå s. 75, Rolls/Treves 
% 					- Skriv om dette; Kva som var bra, dårlig, osv.
% 					- Kunne ikkje gjøre en del handlinger. Førte til pause i ANN-utviklinga fram til tidlig 80 tall.
% 				SKRIV AT desse to ikkje hadde synaptisk vekt. Signalet var boolsk (?)trur eg. Bli sikker på dette først.
%
% 				Så skriv litt om synapser, gå vidare til syn.p. og Hebb:
% 				- Hebb, 1949 (?). Skriv om Hebb rule(?)
% 				- ADALINE ("ADAptive LINear Elements") 								Sjå s. 79, Rolls/Treves
% 					- Brukt for å fjærne ekko fra telefonlinjer.
% 				- Hopfield 90-tallet. (ta med? Ikkje uten at det er direkte relevant..) --Kanskje bare nemne det som folk/funding ble interrester i ANN igjen(Om dette stemmer..)  (?)
% 				
% 				Når ble aktiveringsfunksjonen ulineær? Finn ut, og skriv at dette (i tillegg til multilayered perceptron) gjorde utviklinga aktuell igjen. Forskere ble gira igjen..
% 	
% 				KVA ER FEIL MED DESSE MODELLANE i forhold til biologiske NN?
% 				- Frekvensbasert. Kontinuerlig ulineær funksjon som kan sies å representere frekvensen til neuronet.
% 				- Synkron oppdatering: alle nodene oppdateres likt, i motsetning til den biologiske ekvivalent, som er "event-basert" (asynkron).
% 				- Læring: skriv om læring. Fokus på "Hebbs law", ustabilitet (forberede leser for seinare å lese om stabilitet for spiking ANN)
% 					- Skrive at mange forsøk er gjort på å stabilisere Hebb. Ikkje einaste læringsregel, men einaste "biologically plausible". Stabilitet for læring er eit issue intill no, uansett.
%
% 		\section{ Third generation ANNs }
% 			- Oppdagelsen av STDP satte fokus på relativ spike timing for pre- og post- synapsisk neuron.
% 			- Skrive om SANN. Bedre simulering av nodene (neuron). Sender ikkje signal vidare kvar tidsiterasjon, men bare når depol er over terskel. Basert på modell presented in Hodkin/Huxley, 1952 artikkel.
% 				"Thus, one of the fundamental questions of neuroscience is to determine if neurons communicated by a rate code or by a pulse code.[1]" 
% 					[1]: (Wulfram Gerstner (2001). "Spiking Neurons". In Wolfgang Maass and Christopher M. Bishop. Pulsed Neural Networks. MIT Press. ISBN 0262632217.)
% 			 	- Dette kan eg skrive masse om i forhold til modelle min: Aktivitet er koda med aktivitet ("frekvens"), mens syn.plast. er avhengig av timing på spikes. (?)
% 			- Pga. meir realistiske egenskaper og bedre simulering av neuronet, kan SANN også brukes for simuleringer til neuroscience.
% 				("They have proved useful in neuroscience, but (not yet) in engineering. " (fra wiki))
% 				 "To date, there have been no large scale spiking neural networks that solve computational tasks of the order and complexity of rate coded (second generation) neural networks." (Fra linja under på wiki)
% 			
% 			- SANN 	-  Korleis er dette for SANN?  Skriv at dette er en simulering av enkeltnodene i neurale nett, og at dette skjer i presens (input fører til auka verdi, over terskel fører til output, osv)
% 				- Skjer i presens, planlegger ikkje lenger enn kva som skal gjøres neste tidssteg (kø). Moore automata (basert bare på tilstand for enkeltnodene (depol.verdi)).
% 			- KANN  -  Korleis er dette for KANN?  Skriv at dette er en høgare ordens simulering av ligningene som er resultat av prosessane i enkeltnodene i neurale nett.
% 				- Dette skjer i presens, men planlegger også framtida: er framsynt. Mealy automata (basert på state + inputs).
% 			- Kanskje: Skriv om forskjellane mellom Moore og Mealy automata.
% 		\section{ KANN. }
% 			. Skrive at: dersom målet er å lage eit pragmatisk nyttig ANN med tidspunkt-info for spikes, bør SANN skrives helt om. Skriv om at SANN bruker mykje ressurser på å opprettholde en stabil tilstand i nettet.
% 			- Siden beregning kreves bare når verdiane endres (i biologiske NN), bør dette være fokus for ANN. SANN bruker mye ressurser på å opprettholde en stabil tilstand (dersom det ikkje er endring i nettet).
% 			- Det beste hadde vore å utvikla eit ANN som bare krever ressurser ved endring av input til nodene. Skrive litt fram og tilbake om dette. Ende opp med at vi trenger en Mealey automata? (dersom dette stemmer)
% 			-
% 			
% 			
% 			
% 			
% 			
%
% 		TODO Sjekk ut EDLUT. GPL software for å lage pragmatisk SANN. (brukt f.eks. i robot-styring)
%
%
% 		MULIGHET:
% 			- IDE: Bra for å distribuere utregning over fleire enheter, sammenknytta vha. ethernet (dette er bare en mulighet/ide)
% 				Skrive i så fall: at dette bare gjelder for SANN, der nodene
%
%
%
%
\chapter{Artificial neural networks: background}

% TODO Skrive at oppgaveteksten ber meg om å finne eksisterende modeller som representerer BÅDE fyringsrate og -frekvens. Dette finnes ikkje.
% Tenkte difor å heller gå gjennom historien for ANN, for at leser skal ha eit innblikk i kva som eksisterer av frekvens-ANN og etterkvart fyringstid-ANN.
The assignment description want me to make a review of existing ANN models representing both firing rate and spike timing.
To my knowledge such a model does not exist. 
% XXX Skriv også noke slikt som:
Despite my effort to find such models, none has been found.

To illuminate the different aspects of the new model, different models will be reviewed. 
These models are either of a historical interrest or of a more immediate relevance to this report.

The pragmatic use of simulations of networks of neurons started with the McCulloch--Pitts neuron in 1943. 
Warren McCulloch, an early neuroscientist and matematician Walter Pitts first began a formalized discussion of the mechanisms of the neuron.
The result was the first artificial neural network, consisting of logical gates. These modelled the neuron as boolean devises (off--on response)\ref{MccullochPittsHistorie}. % TODO Vær heilt sikker på at dette står i denne artikkelen.
What is later referred to as the first generation ANN is based on the McCulloch--Pitts neuron\cite{Maass97networksof}.
One of these models is Rosenblatt's Perceptron\cite{HaykinANNbok}.%TODO Finn sitering!
%Neural networks based on the McCulloch--Pitts neurons are also referred to as Perceptron or threshold gates.

The next generation of ANNs have computational units with a continous activation function. 
One common activation function is the \emph{sigmoid function} $\sigma(y)=\frac{1}{1+e^{-y}}$.
As we discover in section \ref{ssecTheNeuron} and \ref{sssecTheActionPotential}, the biological neuron has a boolean carracteristic, in the form of having an on--off output to its synapses % TODO KAnskje skrive ref{secBiol.sys} isteden?
	, where the transmission is a function of the size of the synaptic connection (the synaptic weight). %TODO Ta vekk det med synapsene? XXX Isåfall: Innfør begrepet "synaptic weight" annen plass.
%In the synapses the transmission is dependent on the synaptic connection to the next neuron\ref{sssecTheActionPotential}.
%The biological interpretation of a floating point number propagating through the network can therefore be viewed as ``firing rate interpretations'.
Neural nets of the second generation can threrfore be viewed as ``firing rate interpretations'' of biological neural networks\cite{Maass97networksof}.

%In neuroscience, learning is |||xxx Skriv at synaptisk plasticity er samme som læring. (innføring til det neste, og mulighet til å innføre begrepet "synaptisk weight".
In 1949 Donal O. Hebb proposed a famous postulate, later known as Hebb's postulate:
\begin{quote}
When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process og metabolic change takes place in one or both cells such that A's efficiency, as one of the cells firing B, is increased.\cite{Hebb1949Kap4}
\end{quote}

This postulate were formalized into formulas for synaptic plasticity, and has been extencively used as a learning rule for artificial neural networks.
This has been referred to as ``Hebbian learning''\cite{HaykinANNbok}.
%Hebb's postulate were later formalized into learning formulas for synaptic plasticity. 
%``Hebbian learning'' is still used in some forms for neural networks. % TODO Skriv HEILT spesifikkt / finn referanser.


Synaptic plasticity can be divided in two groups, Long Term Potentiation (LTP) causing an a lasting increase in the synaptic weight and Long Term Depression (LTD) causing a lasting decrease in the synaptic weight.
Gustavsen et. al found in 1987 that the synaptic plasticity vas a function of the postsynaptic depolarization at the time of transmission. 
The synaptic plasticity varied from a strong LTP to a strong LTD as a result from a single transmission\cite{Gustafsson03011987}.
%Dette ble også overført til ANN

We can say that, statistically there is a correlation between the presynaptic depolarization after a synaptic transmission, and the relative timing of the postsynaptic action potential. %ELLER NOKE: RYDD OPP XXX XXX
If we view this statistically, we can say that the timing of the presynaptic action potential (``firing'') in relation to the postsynaptic firing is a direct consequence of the postsynaptic potential at the time of transmission.
Based on this analysis alone, we can say that the relative timing of the pre-- and post-- synaptic firing is fundamental for the size and direction of the synaptic plasticity.
% XXX KVIFOR tidavhengig? XXX : There are also hypothesis about a retrograde signalling mechanism in the neuron that gives the same effect
This recieved the name ``Spike Time Dependent Plasticity''(STDP).


%XXX XXX XXX XXX X   XXX XXX XXX HER HER XXX HER XXX HER XXX HER XXX HER XXX HER XXX HER XXX HER XXX HER XXX HER XXX HER XXX HER XXX HER XXX HER XXX HER XXX HER XXX HER XXX 
%her er eg.


%Because Hebb's postulate only considered poitive weight change, STDP has later been %omtalt som stable hebbian learning.
%As we will see later, a simple formulation based on ``Hebbian learning'' is unstable, because it only states a positive weight change. % As this varies with the activation  XXX Skriv at det er exponentially-ustabilt.
%and many variations has been devised to stabilize synaptic plasticity, many of them biologically unplausible.
After the discovery of STDP, the timing of the action potential recieved an increased focus also in ANN models. 


%TODO NEVN action potential i innledning! XXX VIKTIG!

%Simple variants of ``Hebbian learning'' only give an increase in the synaptic weight, and is clearly exponentially unstable. Different variations of the naive model of synaptic 







The McCulloch--Pitts neuron has similarities with what is later reffered to the third generation ANN, Spiking Artificial neural networks.

This started what later is called the first generation ANN\ref{Kunkle02pulsedneural}. %todo REFERER Pulsed NN and their application

%dette førte til utvikling av perceptron. osv. Skriv om 1.gen ANN. gå over på Hebb:


%xxx Skrive at forskjellen mellom de ulike generasjonene er på noden: kva gjør noden? Kva er input, activation function og output?










\section{Bakgrunn: ANN}
	\subsection{årsak for bruk av ANN / historie}
	
	\subsection{The traditional ANN}
Early ANNs used float values as the value that propagates through the network. According to sec. \ref{secTheBiologicalNeuralSystem} this is not so in the biological neuron.
The biological neuron have a boolean (``all--or--none'') transfer of value. A ``true'' value propagating through a neuron is called an action potential or a ``spike''.

The biological understanding of the float value as the signal is that the value represents the frequency of the neuron's firing in a given time interval.

Modelling the neuron in this way makes it better for simulating ANNs in a computer. 
Instead of a vast amount of bolean signals generated at each neuron and transmitted through its many synapses, the computations are done by operation on variables representing the spike frequency of the neuron.
If the non-linear function at each node is made to aproximate the neurons response to different input frequencies, the theory is that ANNs based on this model aproximates a network of biological neurons.

Aspects of the neuron that are functions of time, however is lost because the simplification removes all information of the relative timing of spikes.
The temporal delay at the axon hillock (generation of a spike), the axon and the synapse (transmission of a spike), temporal leakage of the neurons depolarization value and many other mechanisms in the time domain is at best crudely aproximated.
It is more efficient, however. This makes this kind of ANN popular for algorithms that need associative properties, e.g. pattern recognition. It is seldomly used for simulations of neural systems.

%Neuroscience is a relatively young field, and knowledge about the importance of prevously ignored aspects of the signal is discovered every year. 

		\subsubsection{Synaptic plasticity for the early ANNs}
Since the traditional ANNs (tANN) does not contain information about the phase of the signal, the information about the relative timing of spikes is lost. This implies that STDP can not be calculated for these ANNs. 
The learning rule for tANN is based on a wery simple variant of STDP that does not consider the relative timing of spikes.

In 1949 Donal O. Hebb proposed the famous Hebb's postulate:
\begin{quote}
When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process og metabolic change takes place in one or both cells such that A's efficiency, as one of the cells firing B, is increased.\cite{Hebb1949Kap4}
\end{quote}

This has been an important postulate both for neuroscience and for ANNs, and led to what later has been referred to as ``Hebb's learning rule''.
\begin{equation}
	\delta w_{ij} = \sum{k r_i r_j'}
\end{equation}
Where $w_{ij}$ is the synaptic weight between neuron j and i. $r_i$ is the rate of neuron i and $r_j'$ is the output of neuron j (the input of neuron i). \mbox{k} is the learning constant. See \cite{Hebb1949Kap4}, where Hebb originally postulated Hebb's principle.

Learning in the early ANN is based on a hebbs postulate of learning. %XXX Skriv noke som skal referere: \cite{Hebb1949Kap4}.

Since the frequency is a defined as a positive size, the weight change $\delta w_{ij} = \sum{k r_i r_j'}$ will allways be either positive or negative depending on $k$. 
For any useful learning rule, $\delta w_{ij}$ will sometimes have to be positive. This implies that $k$ is a positive constant, and Hebbs learning rule will for any time iteration be positive
%, which makes the synaptic weight unstable in terms of unlimited synaptic growth.
 . This makes the synaptic weight unstable in terms of unlimited synaptic growth.
Many atempts have been made to stabilize the synaptic growth for neural networks, eg. \cite{hebbUstabilt}. %ikkje skriv "eg.", sjå heller om påstanden er skrevet i artikkelen og referer isåfall dette (da kan eg fjærne "eg.")

One method for stabilizing synaptic weight is based on the concept of STDP discovered from biological neurons in 1987.
Gustafsson et al. proposed that the synaptic weight gain varied with the postsynaptic neurons depolarization after synaptic transmission\cite{Gustafsson03011987}. 
\begin{quote}
Moreover, the finding that homosynaptic tetanization produced little LTP after this pairing procedure suggests that LTP, at least over the time span examined, is controlled by postsynaptic depolarization and does not depend on high-frequency presynaptic activity for induction.\cite{Gustafsson03011987}.
\end{quote}
This has later been known as Spike Timing Dependent Plasticity (STDP). Se \cite{reviewSTDP} for more information about STDP. The biophysiological background for STDP has been described in section \ref{forklaringBakSTDP}.

% figur funker bare for pdflatex. Ta med til leveringa.
%\begin{figure}[htb!p]
%	\centering
%	\includegraphics[width=0.8\textwidth]{figurSTDP.jpeg}
%	\caption{Spike timing-dependent plasticity. a, Synapses are potentiated if the synaptic event precedes the postsynaptic spike. Synapses are depressed if the synaptic event follows the postsynaptic spike. b, The time window for synaptic modification. The relative amount of synaptic change is plotted versus the time difference between synaptic event and the postsynaptic spike. The amount of change falls off exponentially as the time difference increases. In addition, the amount of potentiation decreases for stronger synapses, whereas the relative amount of depression is independent of synaptic size.}
%	\cite{stableHebbVedSTDP}
%\end{figure}

\subsection{Spiking Artificial Neural Network}
The importance of the relative spike timing of the presynaptic vs. the postsynaptic neuron for synaptic plasticity led to development of Spiking Artificial Neural Network (SANN). 

SANN is a simulation of a network of neurons in the time domain. Activity of each node is represented as the neurons depolarization. For this simulation of biological neurons, it is important to calculate the leakage of value each iteration. 
The behaviour of the neuron in the frequency domain follows that of the biological neuron since each node is a simulation of the biological neuron. %For this reason network aspects of the ANN will also aproximate that of biological NN
Simulating each neuron will not be as effective as the traditional ANNs because of simulating every spike, every transmission and leakage of each neurons depolarization every time step. 

SANN use boolean signals intracellularly. At the synapse, the tranmission is given by the synaptic weigth. 

The major advantage of SANN is that it retains imformation about the timing of the spiking for the neurons.  %TODO Ikkje skriv "retains". Finn på noko som gir bedre flyt i teksten.
Because of the advantage following STDP, SANN is frequently referred to as ``third generation ANN''. 

Neural science is a relatively young field % (aprox. 50 years)
	, and the important mechanisms in neural networks has not been discovered fully. %referer eller kutt ut "approx 50 years"
Discovery of Local Field Potential Oscillations (LFPOs) has generated much discussion. LFPOs are oscillations in the activity of local inhibitory neural circuits. 
These circuits give output to a large part of the neurons(local to each inhibitory circuit), and can be seen both on network behaviour and on the individual neuron. 
%kva meiner eg med "following each neuron" ? Skriv bedre!
Whether this is purely a network phenomenon or following each neuron is unknown, but the importance of LFPOs in neural calculations is assumed. %to be [stor]
%referer XXX

%The relative timing of the firing of each node have [blitt mykje større i det siste] Kvar er "det siste". Referer. osv
%Skriv bedre, mykje bedre :
Anyway, the focus of computational neuroscience is to a much larger extent on the timing of the indivitual neural spike. 
%TODO :
Finn ut når timing ble så viktig, og skriv litt meir om dette. Skriv at SANN har blitt populært, og at eg syns det er litt teit å direkte simulere neurona når man har høgare matematikk tilgjengelig.
%Denne tanken min er bakgrunnen for at eg har utvikla KANN. Dette kommer kanskje litt seinare i teksten?



%Skriv tilslutt at simulering av kvart neuron virker lite effektivt. Det leda meg inn på tanken om KANN. Så skriv om KANN.
\subsection{The reason for developing a new model -- $\kappa$ANN}
As a computational system, a neural system is fundamentally different from the processor in a computer. 
The neural system is based on a vast amount of computationally weak, massively parralell indivitual ``processors'' ---the neuron. 
The computational unit of a computer, the processor, is funtamentally different. It is one or a few, computationally strong, serial processor(s). 

In neural systems the neurons are directly connected, such that the output of one node is the input of another. In computers, the result of a computation is saved somewhere in memory and that can be accessed by (one of) the processor(s).

With this fundamental difference betwee  \emph{XXX} SKRIV MEIR HER.



\newpage
		\emph{SJÅ essay i NEVR3004 - rapport}

	\subsection{historie for ANN}


	\subsection{årsak til å gå vidare fra tANN til SANN}
	\subsection{Grunn til å gå vidare fra SANN til $\kappa$ANN. Ide for $\kappa$ANN}
		Kanskje eg skal her skrive om kva eg  reagerte på med SANN? (ikkje vent å sjå på resultatet, skriv uansett. Tolk heller at dette var feilt..)

\section{Artificial Neural Network Architecture}
Kan eg skrive ontrengt som section over, bare for arkitekturen til ANN. Ende opp med recurrent ANN!
Legg ved fig. 5.13 i boka "Recurrent neural networks for prediction" s. 84






