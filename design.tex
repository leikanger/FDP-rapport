% Implementation design :
rart, dette kommer på anna side..


% TODO Denne må skrives heilt om. Sett av to dager til dette. (1 natt+en dag?)


PLAN:
- innledning til kap.
%XXX INNLEDNING til kap. ER FLYTTA TIL rapport.tex (ELLER er planlagt flyttet).
	% Skrive at alt dette er egenprodusert. Alle mekansimer, ligninger osv er egenprod. Dette var unødvendig, da SANN allerede er laget. Det er difor mulig at eg har innført nokre nye element for dette.
		% Siden alt er egenprodusert ("oppfinne hjulet på nytt") har eg ikkje leita mykje om implementasjonsspesifikke detaljer, og det er lite referanser i dette kapittelet.

% 			Først: Innledning FOR KAPITTELET.
% - Skriv at for å gjøre restultata mest sammenlignbar, så tenkte eg initiellt at signalet propagerer ved "fyring" for begge modellane (KANN og SANN). Dette er gjort om.
% - Nevn forskjellane: SANN ser på tilstanden, mens KANN ser på tilstand+input for å beregne fremtidig fyringstid. Dette fører til behovet for pEstimatedTaskTime. Beskriv behovet. Mulig bruk i SANN også..
%  		(dette vil bli sett på i section {secSANN} og {secKANN}).
% - Læring (syn. p.) er ikkje implementert pga. tidsbegrensninger(og  siden dette ikkje er en del av oppgaven).							= "For further research".
% 		- Blir dermed sammenligning av to modeller for å designe `recurrent ANN': "Moore vs. Mealy state maschine of the biological neuron".
% - Siden målet er å beskrive forskjellane i implementasjonen, er det viktig å isolere forskjellane mest mulig.
%  		-> Dette vil bli diskutert i section ? (secSammenligningDesign?)
% - For sammenligning av enkeltnodene er det best å ta bort så mange variabler som mulig. Difor er eit kaotisk neuralnett som input uakseptabelt.
%  		Løsninga blei å igjen hente inspirasjon fra biologien, og lage eit spesialisert "sensor neuron". Dette skal bare nevnes her, og TODO skrives eit avnitt av seinare i kap.
% 		Dette blir designet i section [secSensorNeuronDesign?].

% 				- Til innledning:  Sjå "kladd fra tidligare: effektivitet". Om at det er vanskelig å forutse kva som er effektivt. Eit par gode poeng der. Bør teste effektivitet, men dette får bli "for futher research".


- section design
% 				- About efficiency (om å optimalisere effektivitet. Kø for beregning. (Variant av) avsnitt er skrevet)
% TODO Skriv om sensor funksjoner!
% 				- loggføring - sjå "log for comparison".
% 				- synaptisk plastisitet. Har ikkje tid. Skriv at vi i denne oppgava fokuserer på implementasjonsforskjellen mellom KANN og SANN. Dersom effektivitet skulle sammenlignes er det best å fjærne syn.p. fra ligninga (mindre kaos)
% 				- Skriv om kvalitative forskjeller for SANN og KANN. Forskjellen mellom SANN simulering av neuron og KANN matematisk simulering med estimering til fremtida.







% 	Section: Design.
% 		- Time
% 		- The object design
% 		- Oppbygging av to typer ANN ved arv.
% 		- Om effektivitet
% 		- Om oppsamling av K
% 		- Syn. P.
% 		- Log for comparison
%
%
%
%i subsection "sammenligning" (også noko som må designes..):
% 		%- Tenkte initiellt å sammenligne effektivitet. Dette blir ikkje tid til.
% 		% 	Målet blir heller å utvikle og sammenligne KANN mot SANN, ikkje resultatet av kjøringa av de to (ikkje effektivitet, men andre aspekt..) Dette bør kanskje skrives om.
% 		%- Siden målet er å beskrive forskjellane i implementasjonen, er det viktig å isolere forskjellane mest mulig.
% 		%	- Lager dermed eit generellt rammeverk som gjelder for begge implementasjonane, og modellane som underelement av elementa:
% 		%		Underklasser av kvart element i nodene (SANN og KANN) som arver fra felles i_[element] abstrakt klasse. Dette gjør det lettare å isolere forskjellane (mest mulig av det som er likt ligger i i_[element]).


% TODO Skriv om sensor funksjoner!




Skriv innledning. Kva skal stå her?
\begin{itemize}
	\item Korleis man impleneterer er abhengig av kva som er målet for implementsjonen (likhet til det simulerte systemet("retthet"), effektivitet(pragmatisk), anna? ). (Se design, forrige kapittel)%TODO Skriv om det i forrige kapittel.
	\item Skrive mine valg. Dette er avhengig av kva eg ønsker å sammenligne systemet for. Velger vel den pragmatiske veien(?)
	\item programmeringsspråk (viktig for implenetation design): Objektorientert. Kanskje nemne at eg vil bruke C++ (Skal seinare skrives meir om (i "implementation"))
%	\item Sammenligne læring? TODO Skriv kvifor læring ikkje er med: Målet for denne oppgaven er ikkje læring, men sammenligning av de to modellene. 
% 																				- Dersom eg får tid til å sammenligne effektivitet: bedre å ta bort læring (få vekk en variabel i eit allerede komplekst system)
% 																				- Tid: Har ikkje tid til å implementere læring i dette prosjektet. Referer også til mailen til Hans Plesser: 
% 																					at modellene for imhibitorisk læring ikkje er ferdig modellert, så enda vanskeligare å implementere (må modelleres først).
\end{itemize}


% Innledning RÅKLADD (på nårsk):
	% Kva er fokus for implementasjonen?
		% pragmatisk eller simulering?
			% Skrive at lite er kjendt om kva som er viktig i neural science, så fokus bør være å ha implementsjonen nært det biologiske systemet. Bør også være lett utvidbart.
			% I utgangspunktet pragmatisk, men pga at vi ikkje veit kva som er viktig, gjør det biologisk nært (dette kan også brukes for å teste ut hypoteser i neural science).
		% sammenligne de to modellane: Impelmentasjonen av de to modellane bør dermed være så lik som mulig (uten å gjøre de separate modellane dårligare).
	%


\section{Design} % eller "Design; General Consepts for both Imlementations" eller noke anna..
	% TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO 
	% Skriv en liten innledning til denne section. Det er tross alt litt tekst i den..

	%In this section the common framework of the design and implementation of the two models will be presented.
	%TODO Gå gjennom opplistinga i neste linje, når section er ferdigskrevet.
	This section will start with describing the common framework of the implementations; 
		General concepts such as simulated asynchronicity (sec.\ref{ssecTime}), generality of the implementation and the design of the indivitual node will be presented in this section. %TODO sett på internreferanser til rett subsection.
	After the common framework of the simulation is presented, there will be a short discussion about how the two simulators are distinguished. %TODO skriv om! slutten er dårlig
	%Towards the end of the section, different aspects like efficiancy and generality of the implementations are presented
	Finally, a concept that is important for a comparison between the transient time course of the two models will be introduced is sec. \ref{ssecLogForComparison}. % %"log for comparison" xxx fiks referanse.
	
	
	%The ide is that a simulation based on higher level mathematics gives a more effective execution of the ANN than a direct simulation of a biological system.


	%   (Husk:  etter bra innledning har eg og leseren samme utgangspunkt i følgande subsections).


	\subsection{Time}
	\label{ssecTime}
	The biological neurons does its tasks completely asynchronously.
	%The concept of ``asynchronous'' comes from electronics, where 
	By this it is ment that the different mechnanisms of one neuron can happen totally independently from an other, and the effectivity of one is not affected by the workload of an other neuron.
	This causes the calculation at each node to happen independently of other neurons.
	%By this it is ment that the excitation, inhibition and firing of one neuron is separated from other neurons; %Ikkje "separated", men adskilt
	%	Multiple neurons can do theire tasks simultaneously with no loss in efficiancy.
	%One neuron can therefore be excited to suprathreshold values without affecting the efficiancy of other neurons, and multiple tasks can be done simultaneously.
	%One neuron can therefore be excited to suprathreshold values and fire without affecting the efficiancy of other neurons' calculation. 


	In the computer, tasks are performed serially in one or multiple processors.
	To simulate the same degree of asynchronous behavior, time also has to be simulated. %TODO Skriv om litt.
	The consequence is what will be referred to as simulated asynchronicity.
	%This will in the remainder of this text be referred to as simulated asynchronicity.
	%To get the same degree of asynchronicity in simulated neurons, time will also have to be simulated.

	In digital electronics, asynchronous design refers to clockless circuits. 
	With respect to time, every part of the circuit executes its task independently of other parts.
	%Every part of the circuit executes its part independently of the other, in respect to time.
	This interpretation of the concept is not possible to accomplish in the serial processor; Tasks can per definition not happen serially, independent of the timing of the other processes.
	The solution to this is to let the time in the simulation be simulated.
	As time (``chronos'') becomes a variable that is under our control, it is also possible to control chonology in the simulation. %XXX ".. under our contro." => ".. under the implementation's control."
	This means that the simulator is able to arrange the order of events om theire order of occurence in the simulated time. %TODO ÆRS! Krokete som helvete. KRØKETE SKREVET: Skriv om!
	%It is therefore possible to control the order of the events in the order of theire order of occurence in time, in the simulation.
	
	The consequence of this is that in a simulation of a network of tree nodes, node B could be ordered to happen after node A but simultaneously to node C, in respect to the simulated time. %Litt dårlig. KRØKETE
	As each node has a time variant value, due to ``leakage'', time is an important element of the mechanisms of each node.
	%As each node has a time variant value, with a value that leaks in respect to time, time is a very important element of the mechanisms of each node.
	The concept of simulated time therefore becomes very important for the simulator.  %TODO Litt barnlig språk: VELDIG! Ta vekk "very"?

	The simple solution to the concept of simulation of time is to iterate through all nodes at each time step, and update the value as a consequence of time propagation.
	As efficiancy is important for a neural simulator that is to be used in technology, optimalization makes it undesirable to do the calculation for each node every time iteration.
	In the remainder of this section an alternative will be proposed.

	%A discussion of what time is, is outside the scope of this text, but a discussion of the result of asynchronicity behavior is important to this project.
	
	 	\subsubsection{Simulated Asynchronocity}
	%TODO Feil start på section / Dårlig skrevet. (Det er jo einaste måten å gjøre noko i datamaskina..) TODO TODO Skriv om neste linje.
	One way of simulating asynchronicity in neurons is to use the concept of discrete time, where one time iteration is the smallest time step.
	%If the set of tasks of one time iteration remains constant during the time iteration, and new tasks are at first added to the next time iteration's taks list, we could say that we have simulated asynchronicity.

	%TODO Begrunn utsagnet. "Blabla, as THIS AND THAT"
	If the set of tasks of one time iteration remains constant during the time iteration, and new tasks first could be added to the next time iterations task lit, we could say that we have simulated asynchronicity.


	One way of doing this is in discrete--time systems is to define time iterations by the ``real world'' clock.
	In this case it is important that every task in each time step is executed before time is iterated.
	%As the work load of each time step varies, 
	%DA MÅ VI define each time iteration so large that ALLE BLIR UTFØRT FØR SLUTT AV ITERASJON.
	Using the ``real world'' time as a basis for these time steps will create to strong a dependence on the work load of the system.
	% Alt for oppstykka tekst. Få også med at evt. kan vi ha STORE time steps.
	As this means that is would only work if all tasks is carried out before the iteration ends, such a dependence is undesirable. % this is avoided.
	% 																							% 					ville skapt for store tidssteg, and is undesirable.

	A better alternative is to use a scheme based on serial execution. % ".. a time scheme .."? Må ha med litt meir enn bare scheme..
	This could be based on a linked list containing the different task elements, and having a scheduler function that calls a certain function for each element.
	If each element is a pointer to an instance of a class derived of a common interface class, the scheduler function could call one of the member functions of the interface class.

	If we let the interface class be an abstract class, with pure virtual fuctions, these pure virtual function would also be inherited to the derived classes.
	Unless the derived class defines these functions, it also inherits being an abstract class, and no object if that class can be created \cite{Stroustrup2000KAP12}.
	We can therefore be certain that each object has its own version of the virtual function.

	To concretize this discussion, let us take a look at the mechanisms of the implementation.
	Here, the abstract time interface class is named \emph{class time\_interface}.
	%The class definition can be seen in appendix \ref{appendixTimeInterfaceDefinition}. % ".. can be seen in appendix ?" => ".. is included in appendix ?".  Bedre, syns eg.
	For the sake of completeness, the the class definition is included in appendix \ref{appendixTimeInterfaceDefinition}. % ".. can be seen in appendix ?" => ".. is included in appendix ?".  Bedre, syns eg.

	We can see from the class definition that the two member functions of \emph{timeInterface} are defined as pure virtual functions.
	%This gives that every derived class must first define these two functions before an instance of the class can be created.
	This gives that for every derived class, these two functions have to be defined before an instance of the class can be created.
	As these functions define the class' scheduled behavior, we have that every object of a class derived from \emph{class timeInterface} have a clearly defined behavior through these virtual fuctions.
	%This gives that every object of a \emph{timeInterface} derived class has a clearly defined behavior for these inherited classes.
	%We therefore have a clearly defined behaviour for every element that is derived from \emph{class timeInterface}.

	% Neste setninga var alt for lang. Kort ned/ del opp. ..Kanskje ok no ?
	The function that is responsible for execution of the task of each object in \emph{pWorkTaskQue}, is the virtual function \emph{doTask()}. % derived from \emph{timeInterface}. %TA VEKK: "... derived from \emph{timeInterface}." ?
	This function contains all the actions that are to be made as a result of the behavior that is to be implememnted as being chronological. %XXX Litt rart med ordet "chronological".
	% Neste setning passer ikkje heilt inn, men det er kanskje viktig å nevne den. Finn på noke anna..
	% Tjah.. 
	The function doCalculation() will first become important later, when we discuss synaptic transmission in $\kappa$ANN (sec. \ref{ssecImpOfSynTransmissionKANNN}). 
	%The function doCalculation() will first become important later, when we discuss synaptic transmission for the new model in sec. \ref{ssecImpOfSynTransmissionKANNN}. 
	 

	%TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO 
	% SKRIV OM NESTE BITEN: Blei bare rot.. Kanskje eg ikkje skal ha den her i det heile tatt?
	% i allefall veldig mykje mindre. Også vanskelig å snakke om ting som ikkje er introdusert enda..

	%TODO XXX XXX Dårlig overgang! Skriv noke anna i neste setning:
	The above discussion became very theoretical, and it is time to present an example. 
	% For mykje. Skriv bare : "For the direct simulation of the biological neuron, the signal propagation is strongly causal."
	%For the biological neuron, and the direct simulation of this, the SN, the signal transmission is strongly causal.
	%The \emph{s\_auron} is chosen as the example presented in this section.
	% AVSNITT SKAL VÆRE KOBLA IHOP MED NESTE LINJE

	%XXX Neste linje seier litt for lite i forhold til diskusjonene her.. XXX Føles som eit tomt utsagn TODO SKRIV OM / fjærn.
	For the direct simulation of the biological neuron, the signal propagation is strongly causal.
	A subelement of a SANN node, the \emph{s\_auron} is chosen as the example presented in this section.
	%As the example presented in this section, the \emph{s\_auron} is used.
	%
	% XXX Neste linje er for lang (føles rett, men ER for lang)
	As the we will se in sec. \ref{ssecSANNAP}, the SANN node's \emph{s\_auron} has the role of the axon hillock in the biological neuron, and will be scheduled for execution when the value of the node goes to suprathreshold levels.

	As introduced in sec. \ref{ssecTheActionPotential}, the axon hillock initiates the action potential for the biological neuron.
	Let the action potential be the task of the SANN node's axon, the \emph{s\_axon}.
	The action of \emph{s\_auron} will then be to insert the node's \emph{s\_axon} into \emph{pWorkTaskQue}.
	When each of the node's subelements \emph{doTask()} is defined later in this chapter, we will se that we get a fully functional SN. %Litt krøkete start? XXX 
	%For the reader, the elements' \emph{doTask()} actions are listed in appendix \ref{appendixDifferentDoTaskFunctions}. 					
	For an introduction to the action of each element's \emph{doTask()} it is referred to appendix \ref{appendixDifferentDoTaskFunctions}. %XXX Ikkje heilt fornøgd med starten. "introduction" er feil. TODO Finn rett ord.

	%This element of the node is responsible for initiating the action potential, so the task of the \emph{s\_auron} is to insert the node's \emph{pOutputAxon} to \emph{pWorkTaskQue}.

%	The \emph{s\_auron} has the role of the axon hillock, introduced in sec. \ref{ssecTheActionPotential}.
%	It is also referred to sec. \ref{ssecSANNAP} for the role of the \emph{s\_auron}. %XXX Ta vekk? Det blir kanskje litt mykje internreferering.. (Ta vekk den over, eller denne, i såfall. Finn ut etter at SANN er ferdigskrevet.)
	
%	When the \emph{s\_synapse} does its task, the postsynaptic potential is altered. 
%	As this does not happen instantaneously, it is important to let the action be carried out the next time iteration. 
%	The task done by each \emph{time\_interface} derived class' \emph{doTask()} function is listed in appendix \ref{appendixDifferentDoTaskFunctions}.

% XXX XXX XXX Dersom eg får tid: Ta med de kule illustrasjonane for pWorkTaskQue. Kjempebra illustrasjoner. TODO XXX XXX XXX
% Det er også viktig for å få med poenget at jobbane lager nye jobber. Ellers blir heile diskurs over litt fjærnt..
% 		Evt. : Serial time simulation could be implemented by having one task list for each new time iteration. 
%//{
% 	To make the % TODO Fullfør med å lage eit eksempel av pWorkTaskQue. Med de listene, du veit..
%	\subsubsection{UFERDIG. Ta med lett eksempel om korleis pWorkTaskQue funker}
%When it is time for the scheduler-thread to start a new task, it calls \emph{LIST.front()$\rightarrow$doTask()}. 
%This calls the \emph{doTask()} function of the first element of the list.
%Since \emph{doTask()} is a virtual function, we may have different tasks assigned to each class derived from \emph{[father]}.
%
%If we have a list of scheduled tasks $L = [a, b, c, T]$, where all tasks are pointers to classes derived from a common \emph{[father]}. 
%All tasks may generate new tasks according to the rules of each derived class, and when a task is completed, it is removed from the list.
%
%If we allways pick the first task, we get a ``first in--first out'' (FIFO) que. The next task to be executed will in this example be task $a$. %All tasks can possibly generate new tasks.
%If a generates new tasks $x_{a_1}$, this new task will be added at the end of the list. We get the task list
%
%\begin{equation}
%	\nonumber
%	L = [b, c, T, x_{a_1}]
%\end{equation}
%
%Lets say that $b$ does not generate any new tasks and $c$ gives us two new tasks $[x_{c_1}, x_{c_2}]$. After task $c$ we get the list:
%\begin{equation}
%	\nonumber
%	L = [T, x_{a_1}, x_{c_1}, x_{c_2} ]
%\end{equation}
%
%As the syntax indicates, the element $T$ is a rather unique task. $T$ is a const--instance of the timeIterator--class, responsible for iterating time.
%X XX SKRIV BEDRE: 
%\emph{T$\rightarrow$ doTask()} will have the responsibility for any tasks that have anything with time to do. 
%
%First, it will the iterate the global time variable \emph{unsigned long timeIterations}.
%
%Finally it will append a \emph{self--pointer} to the end of the task list.
%\begin{equation}
%	\nonumber
%	L = [x_{a_1}, x_{c_1}, x_{c_2}, T ]
%\end{equation}
% //}
% XXX Kanskje: skriv at en anna måte å sjå det på er å ha to altererende lister. Nye jobber blir lagt til i den andre lista enn den vi utfører ting fra no.
\newpage

% XXX ANDRE TING SOM KANSKJE ER VIKTIG:
%\begin{lstlisting}
%static unsigned long std::list<timeInterface*> pWorkTaskQue;
%\end{lstlisting}

%	The scheduler function is the global loop of the implementation, and runs in the background. Every time a new task is created by the execution of a task, it is inserted at the end of the list. % at the back of the list. ?

% 	In C, if the data in the elements of a linked list are pointers, any kind of data could be pointed to. 
	%The compile time type cheching of C++ enshure 
%	In C++, pointers to objects of a certain class can only be assigned pointers to objects of this class or classes derived from this class.
%	We can therefore use a list of \emph{timeInterface*} to keep a listing of the tasks to be executed in the present and the immediate future.
	
%	Because a derived class also inherits the pure virtual functions of an abstrakt ancestor class, the derived elements have to define the function for it to be possible to create objects of the class








%TODO Skriv også at siden neuroscience er så nytt felt så er det vanskelig å vite kva som er viktig. Eg prøver å lage implementasjon som lett kan fokusere på andre eller nye aspekter ved neuronet.
%Dette er grunnen til  at eg legger det opp som neuronet, med lett utvidbarhet. F.eks. til meir nøyaktig tidssimulering, nye element i neuronet, ... 
%HER ELLER UNDER subsection{Biologisk nært}
	\subsection{The object design}
	\label{ssecTheObjectDesign}
		Inspired by biology, each node of the neural network consists of numerous subelements(se sec. \ref{secTheBiologicalNeuralSystem} for more about biological neural systems). 
		Each node in the neural web consists of the same elements as in biological neural systems. 
		Each node (``auron'' -- artificial neuron) is comprised of 
		\begin{itemize}
			\item Auron, the core of the artificial neuron. With one output axon and one input dendrite.
			%\item Soma, with an output axon and input dendrite.
			\item Synapses, connected to both presynaptic axon terminal and the postsynaptic dendrite.
			\item Axon, containing the neurons output synapses. %XXX Skriv seinare at dette gir mulighet for nøyaktig simuleing av time delay. Men også rask simulering (best for teknologi (pragmatisk)).
			\item Dendrites, containing the input synapses to artificial neuron.
		\end{itemize}
		
% TODO TODO BARE ROT: TODO SKRIV OM neste par avsnitta!

		%XXX TODO Skriv heller at "av implementasjonstekniske årsaker". Destructor. Beskriv meir i "Biologisk nært"avsnitt. Kall det heller for "Object layout". XXX
		%Lets start with the dendrite. Each object of the dendrite class will have one or more input synapses. It will also need to be able to access the elements of the postsynaptic auron. 
		Lets start with the dendrite. When the dendrite recieves much information through its input synapses, it need access to the postsynaptic neurons activationObject (more on this later). %TODO XXX Skriv meir,og bruk heller \ref{secA}
		To retain the possibility to simulate new theories in what is important in neural systems, the auron will also need access to its dendrites functions and variables. 
		One element of learning (synaptic plasticity) might be the so--called ``backpropagating action potential'', that propagates back to the synapses of the dendrite when the auron fires.
		To be able to simulate such elements of learning, we need a two--way connection between the dendrite and the soma.


		The learning happens in the synapse (the connection between nodes of the network). In biological neural systems, the synapse can be percieved as part of both the presynaptic neuron and the postsynaptic neuron.
		Again, to retain the possibility of retrograde messangers from the postsynaptic auron and to send retrograde signals to the presynaptic auron, the synapse will need access to both the earlier and the latter object in the signal path. To do this the synapse will not be part of either neuron but be an object of its own with pointers to the previous and the next object in the signal path. 
		This also enables us to have an abituary number of synapses connected to the presynaptic axon (part of the presynaptic neuron) and to the postsynaptic dendrite (part of the postsynaptic neuron).
		%litt mykje å beskrive både for dendrite og synapse?

		This leads us into the general design of each auron. In my implementation, each element of the auron will be an object of its own, with pointers to the next and the previous object in the signal path.

		%TODO LAG objekt-diagram (er det noko som heiter? ellers: definer mitt eget diagram(egne regler), og lag diagram, og ta med her.
		% Helst bruk UML sei stavdahl.

		%skrive om implementasjon, her? At dette krever mykje av constructor and destructor

		
		
		
%		BLÆ To implement this in an object oriented programming language, each object of class dendrite will need a pointer to an object before it in relation to the information flow (one of the input synapses),
%			and to the object after it in the information flow (the auron it is a part of). To do this 
		


		\subsubsection{Temporal accuracy}
		\label{sssecTemporalAccuracy}
		The object design in combination with elements introduced in subsection \ref{ssecTime}, we get a framework that can easily be adjusted to different uses of the simulation. 
		
		Pragmatic ANNs are often use ``one compartment models'' of the neuron. %REFERER noke!
		Here, each node only has one element (no subelements). This ignores the time delay of transmission through each subelement. 
		In my implementation this can be easily achieved with letting each node directly call the next nodes' \emph{\small{doTask()}}. 

		%For a simulation of neural network with a focus on the time delay of each element in the neuron, we can use the object model with more elements 
		%For a simulation that does not focus of the time delay of each element of the neuron, the \small{\emph{axon.doTask()}} can call \small{\emph{synapse.doTask()}} directly.
		
		For simulations with a focus on the timing of each element of the auron, we can have a high time resolution, and many elements within each subelement of the auron.
		In e.g. transmission through an axon, each subelements' task will be to add the postelement to the work list,%XXX er uttr. "work list" introdusert i time? wær sikker!
			giving the effect of a time delay of one time step. 
		The axon then behaves as a basic linked list of elements, with a time delay of one time iteration at each link.
		This gives an opportunity to adjust the behavior of the neuron in relation to timing to our needs.

%TODO kan kanskje også skrive noke om at for nye KANN, er det endringa som fører til computational load. Dette gir at vi i prinsippet kan ha så nøyaktig tidsoppløsning som vi bare vil. 
%Einaste jobb som blir utført er funksjonskall (?.doTask() ) og det å legge til neste element i arbeidslista..
%her, eller under effektivisering, eller under "The two ANN models"
%Beste er kanskje å nevne det her, med frampeik til f.eks. "The two ANN models"

%TODO KANSKJE HER? Skriv at dersom vi velger timestep til størrelsen av en refraction time (referer til ssecValueOfAlpha) så vil det (statistisk sett) bli rett å vente utsette vidaresending av aktivitet: ett timeStep.

	\subsection{Object design for the artificial neuron} 								%XXX TODO SKRIV BEDRE!!! 																	XXX SKRIV BEDRE
	The artificial neuron, hereby referred to as ``auron'', consists of the same sub--elements that are described in section \ref{secTheBiologicalNeuralSystem}.
	For the sake of generality, each sub--element of the auron will be an object of its own. If higher temporal accuracy is required, this enables us to insert more linked objects into a linked list of causal actions. %ELLER NOKE

	To achieve the wanted effects from section \ref{sssecTemporalAccuracy}, each sub--element of the auron will be an object of its own. 			  %Eller "to achieve generality" ?
	If each sub--element is a linked list with $n$ links, and the task for each intermediate element (between the first and last element of the auron element) is to add the next element of the linked list to the task que, 
		time delay will be decided by the indivitual inplementation of the general framework ($n$ links gives a trasmission time of $n$ time steps).
	For further development of this ANN as a neural simulator, the axon could give different time delays for different synapses along the axon, depending on the synapses distance from the neurons soma.

	In a neural simulator it would be desirable to increase the spatial accuracy for the artificial neuron. 
	Each biological neuron have a multitude of dendrites. This is also ignored in most implementations of ANN for the sake of efficiancy.

	There are hypothesis in neuroscience that suprathreshold depolarization over the membrane somwhere in the neuron may send an intracellular message to the axon hillock that will initiate an action potential as a result.
	% Kanskje: Skriv "theory" og i såfall REFERER!
	This may explain the apparent random behaviour of the action potential as the neurons value approaches the firing threshold. %SKRIV BEDRE. Skriv at det ER appearent random behaviour først. XXX
	If all the input enters at one place in the neuron (one dendrite compartment), a level of input that would not induce firing of the neuron if the input was more distributed on the neuron.
		
	An object design that enables us to expand the number of dendrites if the objective of each implementation is a neural simulation. 
	With this general framework it is also possible to make an effective ANN--implementation by making fewer sub--elements of the auron.

	The object design of also enables an implementation of one neural system to have an abituary spatial accuracy of the axon. 
	These two aspects (spatial and temporal resolution) of the axon and the dendrite makes the implementation good both for neural simulators and for the use of more pragmatic ANN tasks.

	For pragmatic ANNs, each node can have one dendrite. This will be more effective,
	For a simulation of neurons, the number sub--elements (dendrites and axon--links) should be set higher to give better spatial resolution and temporal accuracy. 
	This will give a better simulation of biological neural systems, because of less abstraction the simulated system.

																																													% XXX SKRIV VEDRE, TIL HER
	
	%Also implementation specific reasons exist. XXX SKRIV DETTE FØRST
	This design introduces a some new challanges for the implementation. 
	If each \emph{soma} has one pointer to a dendrite object and an axon object (located in the free store), destruction of one of these objects (e.g. the axon) will need to signal the \emph{soma}. 
	The \emph{soma} is then responsible to take action to avoid memory exhaustion and undefined behavior following calls to the deleted object. %få med at det er peiker-kall, og obj. er sletta
	For this reasons, the pointers between the sub--elements of the auron will be bidirectional. 

	This structure introduces a new problem, the possibility of access and polution of the general design. 
	If the pointers \emph{pOutputAxon} and \emph{pInputDendrite} are protected, the encapsulation protocol of C++ will fix the problem, but now the variables are only accessable to the object or friends of the object.
	The associated \emph{axon} and \emph{dendrite} objects are generated in the \emph{soma::soma()} constructor, and can only be altered from an other class that is part of the auron class--cluster. %XXX skriv annaleis: auron class-cluster.
																															%Skriv utbrodert om kva friend er, og at alle classene i auron-gruppa er venner => "auron class-cluster".

	For encapsulation within this new auron--structure, consisting of soma, axon, dendrite and synapse, we define these classes as \emph{friend} of eachother. 
	Only classes that are member of this cluster will then be able to access and write to \emph{protected} variables.
	%\emph{class axon;} and \emph{class dendrite;} will then need to be defined as a friend of \emph{class soma;}. In the constructor 


	%In addition, the need for retrograde signalling (signalling the other way) may become important for later uses (e.g. for neural simulation).

	


	\subsubsection{Construction of auron elements} %TODO SKAL DENNE subsubsection være her? Plasserte den bare her siste minutt før eg drar å buldrer..XXX
	%XXX XXX XXX XXX 
	%VÆR SIKKER PÅ AT SUBSUBSECTION SKAL STÅ HER / at teksten under gjenspeiler overskrifta.. XXX
	In the construction of a new sub--element, a pointer to the previous element will have to be supplied. 
	The constructors of each class are therefore defined with a pointer to the previous element in the signal path. E.g. the constructor for an axon will make a pointer to the associated soma. 
	When \emph{axon::axon(soma* pSoma)} is called, the soma--pointer will be assigned to the aurons constant \emph{axon::pElementOfAuron} pointer.
	A pointer to the axon is also assigned to the aurons \emph{soma::pOutputAxon} pointer by generating the axon in the free memory and saving its pointer.
	Because \emph{axon::pElementOfAuron} is a constant, it can not be altered. For \emph{soma::pOutputAxon} and \emph{soma::pInputDendrite} however, poiners can be changed, but only from other objects inside the auron--cluster.
%	We can say that we have a wider form of encapsulation (encapsulation to the objects of the auron--cluster).

%	The same applies to \emph{pInputDendrite}.

\begin{lstlisting}
 auron::auron() : tidInterface("auron")
 {
   pOutputAxon = new axon(this);         
   pInputDendrite = new dendrite(this);
 }
\end{lstlisting}

	The synapse however is different, because of the multitude of synapses associated with each auron. 
	\begin{equation} \nonumber
		\begin{split}
			\text{\small{synapse::synapse(}} &\text{\small{auron* pPresynAuron, auron* pPostsynAuron,}} \qquad \qquad\\
													&\text{\small{const bool bInhibEffectArg =false, float fWeight =1);}}
		\end{split} %feiltolking. Dette er rett
	\end{equation} %feiltolking. Dette er rett
			
%\begin{lstlisting}
%synapse::synapse(auron* pPresynAuron, auron* pPostsynAuron, const bool bInhibEffectArg =false, float fWeight =1);
%\end{lstlisting}
	The constructor for \emph{class synapse} takes pointers to the presynaptic auron and the postsynaptic auron as arguments, and assign the synapse to the right sub--element of the two aurons. 
	The last two arguments have a default value, and \emph{const bool bInhibEffect} will be generated as a constant with the value sendt in or the default value (\emph{false}). This defines the synapse as an exitatory synapse. 
	The synaptic weight will be initialized to \emph{fWeight}. Due to learning \emph{fWeight} cannot be a constant.



	\subsubsection{Destruction of auron elements}
	Destruction of one object causes the every pointer to it to be invalid. 
	Both to avoid undefined behaivor following calls to deleted objects and avoid memory leakage, well defined destructors are neccesary for the classes involved in the auron--cluster class.
	%Because of the object model of the auron simulator, destruction of an object therefore should be responsible to remove every pointer to it.

	For destruction, it is best to start with the elements containing only two pointers, one to the pre--element and one to the elements post--element, where destruction does not cause further destruction; the synapse.
	When \emph{synapse::$\sim$synapse()} is called, the pointer to the synapse is removed from the presynaptic axons \emph{pUtSynapser} list and the postsynaptic dendrites \emph{pInnSynapser} list. 
	Both containt only pointers so an infinite recursive destructive loop is avoided. %Skriv noke anna enn "is avoided".

	%kanskje skrive "listed in \emph{std::list<synapse*> pUtSynapseri} ..." ? :
	When the dendrites or the axons destructor is called, every synapse--element associated with it (listed in the \emph{pInnSynapser} list for dendrite and \emph{pUtSynapser} list for axons) is deleted. 
	Removal of an element of std::list<?> usually calls the elements destructor. Since the list only contains pointers, the destructor must be explicitly called by calling the delete operator on the dereferenced pointer (object).
	%TODO skriv slutt av forrige setn. bedre. : on the dereferenced pointer, eller value of the pointer, eller ?

\begin{lstlisting}
  while( !pUtSynapser.empty() )
  	delete( (*pUtSynapser.begin() );
\end{lstlisting}

	An auron only contains one dendrite pointer and one axon pointer, so destruction of an axon can will call destruction of these two elements.
	Destruction of this axon will induce destruction of every synapse element connected to the axon. This will remove all postsynaptic pointers to this synapse, and every synapse pointer at the postsynaptic dendrite is still valid.
	The same goes for destruction of the aurons dendrite.
	


% Bedre å accessere tidligere elements mdl.variabler enn å sende som argument. Skriv om dette.
% Skriv om at det er kanskje raskare å accessere mdl. variabler i de andre elementa av auronet enn å sende alt vidare som argument i funksjonskalla (argument må også construeres, kopieres, osv), 
	% - Fører til mindre å gjøre ved funksjonskall. Trenger ikkje kopiere variabel og sende kopien inn i funk (slik det vanligvis blir gjort). Trenger bare endre pElementAvAuron->aktivitetsvar., så å si fra til postElement at: dens tur
	


	\subsection{Forberede implementasjon for å sammenlikne The two ANN models}

	For comparing the two ANN models, one implementation of each model is created. 
	For the best result, it is important that the implementations have a balance between being as simular as possible and optimalizing each implementation.
	%Meir om sammenligningsmetoder (kva som sammenlignes..) seinare.

	The framework described so far in this section will be the general framework for both implementations, both in respect to time (sec. \ref{ssecTime}) and to the object model (sec. \ref{ssecTheObjectDesign}).
	Other operations, funcion design and at least the object design should do an equal amount of work or use an equal amount of time, depending on what is compared. %SKRIV "MEIR OM SAMMENLIGNING SEINARE"?

	Each element of the auron should also have many similarities. The propagation of the signal from the auron, through the axon and all the output synapses, to the postsynaptic dendrites, etc. will be the same for the two models.
	The difference lies in what propagates through the graph, and how the activity level of each node is calculated. 
	
	The object model will be the object model described earlier (se sec. \ref{ssecTheObjectDesign}), but each node element (dendrite, axon, synapse, auron) will have the design %inheritance design ?
		described in fig.? %TODO TODO Lag denne figuren. UML om klassene for auron--element. (generelt, eller dendrite)
	Each of the elements of the auron will have one abstract class that derives to the model--specific classes of the auron. 
	The abstract class will be denoted i\_[element], and the model--specific classes will be called s\_[element] for the SANN--node, and $\kappa$\_[element] for the $\kappa$ANN--node, 
		where [element] is any of the auron sub--element (dendrite, axon, ...).

	In the rest of this section I will focus on the mechanisms of the dendrite. This is an important element of the auron, and has many differences for SANN--nodes vs. $\kappa$ANN--nodes. 
	It is important to stress that this is but an exaple, and that the %Skriv heller "simular ? goes for .." enn "the same goes for .."
		same goes for the other elements of the artificial neuron.

	\subsubsection{The abstract i\_dendrite class}
	Each dendrite class ( s\_dendrite and $\kappa$\_dendrite ) is derived from the abstract i\_dendrite class.
	This class is a abstract class, and is thus not possible to make an instance of. The class is however inherited to the model--specific classes s\_dendrite and $\kappa$\_dendrite.

	For this reason, the variables and functions that are general for the model--specific classes should be placed here. 
	Both of the model--specific dendrite classes will have a [next element] pointer, \emph{pElementAvAuron} and a [previous element] pointer. For the dendrite the previous element will be an array of all the input synapses.

	These elements will be pointers to the previous and the next elements in the signal path, and should be pointers to object of the right class. 
	For SANN dendrites, the \emph{pElementAvAuron} will be a pointer to an s\_auron and for $\kappa$\_ANN nodes it will be a pointer to an $\kappa$\_auron. 
	To achieve this for the abstract i\_dendrite class, the variables will be of type \emph{i\_[element]*}. %, in the case of i\_dendrite's pElementAvAuron that is a pointer to an object of class i\_xaxon.

	For the model--specific node element, e.g. s\_dendrite, the pointer should point to a SANN element, s\_dendrite. In C++ it is possible to save pointers to derived classes in a pointer variable to the ancestor class.
	This means that a pointer to an s\_auron is possible to save to an i\_auron* variable.

	The best way to implement this is to have the constructor of the model--specific class assign the pointer to the next element in the signal paht. 
	For s\_dendrite this means that the \emph{s\_dendrite::s\_dendrite()} will assign a pointer to an s\_auron object to pElementAvAuron.

	The s\_dendrite constructor take an auron pointer as an argument. 
	Because the s\_dendrite is constructed only from the constructor of the auron element, the s\_dendrite contructor is called with a \emph{[this]} pointer from the aurons constructor.

	The abstract class should have all the functions and variables that are general for all the derived classes. 
	The functions for the dendrite class are \emph{virtual void doTask()}, \emph{virtual void newInputSignal( int )} and \emph{virtual void axonTilbakemelding()}. 
	All of these are defined as pure virtual classes in i\_dendrite.

\begin{lstlisting}
  virtual inline void doTask() =0;
\end{lstlisting}

	\subsubsection{The model specific s\_dendrite class}
	
	\emph{s\_dendrite::newInputSignal( int )} is called from the input synapse in the case of transission. 
	To aproximate the synaptic delay, \emph{s\_dendrite::newInputSignal( int )} is called directly from the synapse at the time of transmission.
	If the postsynaptic node is exited above firing threshold, the dendrite is added to the back of the working que by the command \emph{time\_class::leggTilTask( this )}. 
	This will induce the actions of a dendrite after one time iteration.
	% for å simulere litt tids--delay også i dendrite. Skriv at synapse--klassa bruker litt tid og dendrite--klassa bruker litt tid. For nøyaktigheten til pragmatisk ANN er dette bra nok (skriv at det er vanskelig å vite kva kva skal være)

	%Skriv om kappa--dendrite også her?

	For the s\_dendrites newInputSignal(int) the work done is a simulation of a leaky integration of the input. 
	The simulation of the leakage is implemented as an exponential function, $l^n$ where $l$ is the leakage factor at each time iteration and $n$ is the number of time steps between [now] and the last time the value changed (after input).
	For SANN nodes, the time of input therefore needs to be saved for the use of calculating the leakage. An alternative approach, used by many neural simulator tools, is to calculate the leakage for each node at each iteration.
	
	The calculation of the leakage is implemented in \emph{s\_dendrite::calculateLeakage()}. This is one function that only is present in the SANN variant of the nodes. 
	Because the pointers to the elements are pointers of to the abstract ancestor class (located at the other nodes with direct contact with the node), calls to the model--specific functions and variables will give errors at compilation.
%neuroElements/neuroElement.cpp:443:21: error: ‘class i\_dendrite’ has no member named ‘calculateLeakage’
	\newline \emph{error: 'class i\_dendrite' has no memeber named 'calculateLeakage'}

	One way to avoid this error is explicit type convertion (often called `casting') from \emph{i\_dendrite} to \emph{s\_dendrite} for calls to model specific functions.
	Casting will work wery well, if every call to model--specific functions and variables is from other node elements from the same model.
	Stroutrups advice is to avoid explicit type convertion\cite{Stroustrup2000KAP6}. 
	
	An alternative approach will be to access the function and variables of the model--specific classes only internally in the class. 
	External calls should only be made to functions declared in the abstract ancestor class \emph{i\_dendrite}.

	In this implementation, we have a special situation. We have a sort of a ``meta--class'' for each node. Each node is comprised of four sub--classes; Axon, Auron, Dendrite and Synapses. 
	Three of these classes can be seen as internal to the node, while the synapse could be said to be a connector between two nodes.

	To access the model--specific functions and variables internally in the node, inherited from the i\_[element] demands casting of the pointer (from i\_auron to s\_auron og K\_auron). 
	An alternative approach is to overwrite the pointer to the next and previous element in the signal path with a pointer to a model specific element.
	This will best be described by an example:

	For the s\_auron class, we have one inherited pointer to the previous element and one inherited pointer to the next element in the signal path.
	The pointer inherited from i\_auron pointing at the next element in the signal path, the \emph{i\_dendrite* pInputDendrite}, is overloaded to a pointer to a modelspecific \emph{s\_dendrite} class in s\_auron.
	When calls are made to \emph{s\_auron.pInputDendrite}, every function and variable available in \emph{s\_dendrite} will be accessable.
	This gives us the oppurtunity to access the model specific functions and varibles in the aurons dendrite whithout defying stroustrup's advice.

	At the same time, we still have the oppurtunity to access the pInputDendrite from a pointer to i\_auron, which is the pointers used in the different containers (e.g. the \emph{static i\_auron::pAllAurons}).
	
	As previously mentioned, the synapse is somewhat different. 
	The synapse is between the presynaptic and the postsynaptic node, and can not be said to belong to either of these. 
	Intuitively the synapse should not belong to the presynaptic or the postsynaptic node, but for implementation specific reasons this is done.
	%Skriv om at dersom ... Vettafaen kvifor det er slik. Eg bør enten gjøre om kode, tilbake til å ikkje ha modellspesifikke peikere i synapse, evt. ikkje nevne forskjellen mellom andre element og synapse, her i rapporten! XXX


%TODO HER HER EHR HER HER HER TODO Gjør det over! Godnatt.



	% TODO DÅRLIG SKREVET. SKRIV OM :
	Functions that is localized only in one of the model--specific classes, like \emph{s\_dendrite::calculateLeakage(int)} is only called only from within the \emph{s\_dendrite} class (before addition of the new synaptic input to the value).
%/****************************************************************************/ SKRIV OM: tekst og tilhørende kode.
	% TODO TODO TODO TODO SKRIV om. Har endra meining. Mellom auron og dendrite er det greit (siden alle dendritter er laga i constructor for auron er ALLE auron-dendrite par alltid av samme type).
	% Skriv heller det. Det som står under gjelder fortsatt, men for andre underelement i ANN nodene.. TODO endre også dette i kode:
	In addition the \emph{s\_dendrite} contains a boolean variable called \emph{bBlockInput} that blocks synaptic input to the node. 
	This variable is set (\emph{bBlockInput = true}) when the dendrite caculates that the nodes value goes to suprathreshold values. In addition a pointer to the \emph{s\_dendrite} will be added to the time\_class::pTaskArbeidsKoe list.
	This calls \emph{s\_dendrite::doTask()} the next iteration, which will add the next element in the signal path to the work que.
	Depending on the length of the absolute refraction period for the node, the bBlockInput will be reset (\emph{bBlockInput=false}) either in class s\_auron or in class s\_axon (one or two time iterations later).
	Because of the choice to avoid explicit type convertion, bBlockInput will be reset internally in \emph{s\_dendrite} by a call to a derived function from i\_dendrite, \emph{void feedbackToDendrite()}.
%/****************************************************************************/

%Kanskje skrive at internt i meta-objektet som utgjør kvar [node] kan dette trygt gåes ut fra. 
%For å gjøre det litt trygt, lages de modellspesifikke variabler og funksjoner [private], slik at bare andre klasser og evt funksjoner som er [friend] kan accessere dei. Dette innebærer bare andre klasser i metaklassen som utgjør kvar node
% skriv det først, så hopp over neste avsnitt, og gå rett på kva casting som skal brukes (static_cast).

	Because the dendrite always is constructed in the construcor of the associated auron object, the two can safely be assumed to be % modelspecific elements 
		of the same model (KANN or SANN). %TODO Skriv om setninga over. Blir mange "assume" her. Funker best under (?)
	This is so certain that casting (e.g. from i\_auron to K\_auron) can be assumed safe.

	In C++, the \emph{static\_cast} operator can be used to convert related types, such as a pointer to an object of one class to an other in the same class hiriachy. 
	This will be safer that C style casting (or the \emph{reinterpret\_cast} in C++), that can convert a pointer to one class to some other totally unrelated class or variable.
	To follow stroustrup advice, %og kjøre compile-time type cheching når eg caster, så [det under]. Skriv litt om kvifor eg ikkje bruker dynamic cast.
	the safer \emph{static\_cast} will be used whenever explicit type convertion is exercised in this project.

	It is even safer to use the \emph{dynamic\_cast} operator when casting. \emph{dynamic\_cast} will provide run time type identification whenever casting. %litt rart å sei ".. whenever casting". Casting: kall noke anna, eller fyll ut.
	There is a small run--time cost associated with the \emph{dynamic\_cast} operator. 
	\emph{static\_cast} relies on altervative (compile time) methods to be certain that the type convertion is valid, and run time cheching in \emph{dynamic\_cast} is seen as redundant.
	For ANN, the small run--time cost at each node will sum up to a significant amount.
	Because of the above elements, the \emph{static\_cast} operator will be used rather than \emph{dynamic\_cast} or \emph{reinterpret\_cast} in this implementation.

% TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO
% opp til ********* boksa skal alt skrives om. Har gjordt det om i koden, slik at istaden for å bare bruke i\_[element] funksjoner, og isteden for å bruke eksplisitt casting, overskrives pre-signal og post-signal element fra 
% 	i_[element] (arva fra i_*) med den modellspesifikke s_[element] eller K_[element], avhengig av kva modellspesifikkt element det er snakk om..
% TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO
% DERMED: Skriv om all teksten opp til *****-boksa! (men bruk gjerne en del som eksempel eller noke)




	




	%***********************************************************************************************************
	%Because the objective of this project is to compare two ANN models, the two implementations should be %relativt
	%simular. It is also important to make each implementation optimal, but the framework of each implementation should be simular.

	%For each auron element (synapse, dendrite, axon and auron), I have designed one interface class that derives to s\_[element] and K\_[element]. 

	%All the general functions and variables of each element lies in i\_[element]. Specialized elemens (s\_[element] and K\_[element] has model spesific funcitons and variables for SANN nodes and $\kappa$ANN nodes.

	%Hadde lenge problemer med dette, siden container for element (som dendrite::pElementOfAuron, auron::pOutputAxon, synapse::pOutputSynapser osv.) var av typen i\_[element].
	%Dette gjorde at eg ikkje kunne benytte variablene på tross av elementa (f.eks. s\_axon::doTask() kunne ikkje resette s\_dendrite::bBlockInput--flagg (gjort i axon for å få refraction period).
	%Det er fristende å explicit typekonvertere i\_[element]* til s\_[element]* for element innad i eit auron. 
	%Dette ville nok vore raskeste løsning, men stroutrup advarer mot dette. (finn referanse i stroustrup--boka).

	%Mi løsning ble å kun accesere de modell--spesifikke variablene internt i kvart auron--element, og bruke interne funksjoner (som er definert i $<<$interface$>>$'et for elementet (i\_[element]). 
	%--Bruke bare slike interne funksjoner (i kvart auron--element) til å skrive og lese fra modellspesifikke variabler og kalle modellspesifikke funksjoner  for kvart auron--element.

	%Dette gjør at eg kan bruke i\_[element]* for alle containers i programmet, og koden blir lik for implementasjonen av de to modellane..

	%%\subsubsection{SKRIV OM activityFunction} TODO

	
	\subsubsection{Oppbygging av to typer ANN-noder vha arv}
	Skriv om korleis arv for auron-element gir de to ulike ANN-node--typene. \emph{i\_auron arver til s\_auron og K\_auron} osv.

	---Ja, kanskje skrive enda meir om dette, evt fjærne dette subsubsection..
	


	\subsubsection{Propagation of $\kappa$}
	%Skriv at det er vanskelig å vite når kappa skal propagere. Skriv litt rundt dette: kva som er intuitivt (ved fyring), beste er å ha transient overføringsfunk men dette krever transferfunk. fokus, 
	%umulig å vite kva som gir godt nok resultat, så bør gjøre dette eksperimentellt. : sammenligne depol.-forløp, og sjå når de ikkje lenger er lik. Kanskje etter en ekstra periode, lenger tid, anna? 
	% 	Skal ivertfall vente ei stund. Resultatet om KANN er meir effektivt er avhengig av svaret på dette spm.
	% KANSKJE DETTE IKKJE SKAL STÅ HER? VEIT IKKKJE KOR..
	%ELLER: kan skrive resultatet her, etter at eg har sammenligna resultata..
	For $\kappa$ANN it is hard to know when the activity variable, $\kappa$ should propagate. In many ways, $\kappa$ gives the general input of a node.
	$\kappa_{ij}$, node $j$\,s influence on node $i$ is given by the period of node $j$, given by $\kappa_j$. 
	It is wery hard to deside some specific time instance $\kappa_j$ should propagate, since the period when $\kappa_j$ is changed also influences $\kappa_i$. 
	The period of neuron $j$ and thus the period between synaptic transmissions to neuron $i$ can also be said to be given by the new $\kappa_j$ first after the first period with the new $\kappa_j$.
	
	The best is to make a transfer function from $\kappa_j$ to $\kappa_i$ with a transient time course. 
	Due to time limitations, this will unfortunately be outside the scope of this project, since this will further alter the postsynaptic nodes after node $j$. 
	A neural network of transfer functions, where each node represents a transform, will probably give a better result both in terms of efficiancy and in terms of the transient time course of each node.
	
	What can be established from the above discussion is that the time of propagation should be somewhere between zero and two periods after updating $\kappa$, depending on when $\kappa$ is updated in the period.
	Because of the caotic and unpredictable nature of neural networks, and the simplification to deside some constant time delay, the time delay between updating $\kappa$ and propagation of $\kappa$ have to be found experimentally.
	As the effectivity of $\kappa$ANN is strongly dependent on this aspect, it is wery hard to theoretically find the efficiancy of the new model.

	To experimentally find when $\kappa$ propagates, the transient time course of each neuron have to be compared.
	For pragmatic ANNs, it is also hard to find a measure of ``good enough'' for the comparison of the two plots. 
	SKRIV MEIR NÅR RESULTAT DUKKER OPP! %TODO
	%TODO Når eg har sammenligna de to, skriv meir her. Bra nok kan være "rett" output for ut--noden, eller "rett" transient time course for ut-noden, osv. Sammenligner heile tida med den andre modellen som om den var rett.
	% Dette kan også være feil. Kven veit.. Mykje å skrive på diskursjon etter dette aspektet!






	%TODO uferdig subsection! XXX





\subsection{About efficiency}
When it comes to what algorithms are effective in terms of computational load, it is impossible to generalize across hardware.
One set of hardware may be as effective, or in some cases even more effective on floating point operations, than ???. %TODO cite !
Optimalization of ANN to a particular set of hardware is in many aspects a differet project. 
The aim of this project is a general comparison of ANN based on two different models.
To make the comparison general, we should therefore not be to concerned with the execution time, since the execution time spent on different operations wary greatly between different hardware.

It is however important to optimalize the code for both of the two ANNs. 
For the SANN the leakage (of the nodes value) will be calculated each time the node gets input.
BLABLA BLA. SKRIV OM SANN OPTIMALISERING.
% TODO Skriv om tid, effektivisering i SANN osv., her..


When it comes to the $\kappa$ANN, the leakage is not a concern, and is already implemented in the value equation that is the basis of the model (se section \ref{secMatematiskModelleringAvBioNeuron}).
The leakage in each nodes value is the background of the non--linearity of the equation, and no further calculation if required for this.

\large{HER}

%TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO
%FJÆRN!
% NEI TODO ENDRE: skal handle om calculation task que (ref{ssecCalcultaionTaskQue}).. %
% TODO Endre navn til pCalculationTaskQue, eller noke..
\subsection{Om oppsamling av $\kappa$}
\label{ssecCalcultaionTaskQue}
% Skriv om lekkasje, slik at lekkasje gjennomføres i s_auron::doCalculation(). Da kan eg skrive om pCalculationTaskQue generelt her, og seinare i implementation_KANN.tex skrive om kappa-oppsamling.
To do the minimalize the number of times $p(\kappa)$ is calculated, it is best to wait to after the current time iteration before calculating the result of the new $\kappa$ in terms of the nodes period.
%Skriv heller noke slikt som :  Når kappa er ferdig oppdatert, dvs på slutten av current time iteration.
This is best done by collecting all the calculations in a list and at the end of the time iteration, after all the change of input level at the node is registere, do the calculations.

The most convenient way of doing this is to use the standard library [set]. %Skriv at det antagelig også er mest optimalt: godt optimaiserte funksjoner i std!
\emph{std::set} is an associative container where the key is the same as the value, with unique elements. %Ref stroustrup kap 17 (bl.a. s 491 og 480 under "map" (ca samme greia står på s. 491)
This means that the set will keep the list of collected computation tasks so that every task only appear once.

The \emph{std::set} is optimalized for maintaining a list with a unique key, and for efficient access to the list's elements. 
A better container for the calculation que is \emph{list}, optimalized for insertions and deletions on either end of the list. Random access of the \emph{list} elements are painfully slow\cite{Stroustrup2000KAP17}.
Because the \emph{std::list} is better suited for pCalculationTaskQue's task, \emph{std::list} will be used.
The uniqueness of each element is manually coded in \emph{time\_element::calculateTask()}.

If we define \emph{std::list$<$timeInterface*$>$ pCalculationTaskQue} and let time\_class::doCalculation() be responcible for the uniqueness of every element and to perform the calculatons listed in pCalculationTaskQue, 
	the calculaton of every node with an updated $\kappa$ will only be done once for each time iteration.
These mechanisms means that for every time some nodes $\kappa$ is updated, a [\emph{this}] pointer can be added to pCalculationTaskQue without fear of duplicate entries. 
At the end of the current time iteration, \emph{time\_class.doCalculation()} will perform the calculations required by the elements listed in \emph{pCalculationTaskQue}.    % (once for each entry).




\subsection{kladd fra tidligere: Effektivitet}
Skriv om at det er veldig vanskelig å forutse kva som vil være effektivt. Ulike hardware aritekturer gir  ulikt resultat. Dette er grunn til at eg bare teller antall operasjone. Dersom van lager spesial-hw, så vil det det er spesialisert for gå fort og anna gå seinare..

Skriv også om generellt optimalisering:
\begin{itemize}
	\item flytande tid. Ikkje alle noder trengs å sjekkes kvar iterasjon. (sjå/edit det over). (SKRIV OM implemenasjonene til Ås, og at de bruker synkron tid).
	\item samle opp beregning av $\kappa$ til slutt. $\kappa$ kan endre seg fleire ganger i løpet av en iterasjon. På grunn av dette skal alle noder som får endra aktivitetsnivå legges inn i ei liste. Denne lista gåes gjennom ved tidsiterasjon (i tid::doTask() ). Skal bare ligge eit element av eit objekt i lista, så dersom $\kappa_i$ endres fleire ganger for neuron $i$, vil det bare bli en kalkulering av depol./`interspike period' per [neuron, tidsiterasjon].
\end{itemize}

Her kan også ``Firing Cycle'' stå. Dette er estimering fordi hastighet ikkje er konstant lik gjennomsnittshastighet. Dersom FC skal brukes (som optimalisering) så kan enten gjennomsnittshastigheta over heile perioden brukes, eller så kan perioden deles opp i $n$, og gjennomsnittet i kvar bit brukes (for eksempel $n=2$: deler farta inn i to deler. Først er farta stor, så blir den mindre, fordi $(1-e^{-at}$ flater ut..)

%	\subsection{valg av programmeringsspråk for implementasjon. Kvifor.} % Kansje dette skal være seinare. Tenkte etter meir generelle implementasjonsmetoder (tid, optimalisering (ny tid vs. synkron modell), tidsdelay ved kø, ...)
%		\subsubsection{C++ :concurrency kommer.} % Nevn artikkel koch fann om concurrency i library.}
%		\subsubsection{objektorientert. Skrive subsubsection om dette?}



	\subsection{synaptisk platisitet}
	Skrive at siden eg velger å lage recurrent ANN kan eg ikkje bruke backpropagation eller supervised learning. Må starte fra scratch, og lage en modell for reinforcement learning som er basert på biologien. 
	Fortell om dopamin, diffuse modulatory system, dopamine blir utløst når biologien gjenkjenner pos resultat (mat, sex, trening, glede). Forsterker de synapsene som var aktiv før pos resultat. 
	Lage reinforcement learning basert på dette!

	Dersom eg får lov å utvide FDP til å være FDP+diplom kan eg gjøre det direkte. Evt blir dette neste prosjekt. Skriv dette isåfall..




\subsection{Log, for comparison}
\label{ssecLogForComparison}
To evaluate the transient time course of the nodes values, the best is to write the variables we are interrested in to a log and later evaluate this log in software specialized for this.
This enables us to run the ANN for the desired duration. 

The software is implemented with arguments, and it is possible to deside the number of time iterations at each program call.
The call \emph{./auroNett.out -i 100} will run the ANN for 100 time iterations.

The log is implemented by writing the interresting variables to \emph{.oct} files. This is implemented by having a private std::ostream member in the \emph{i\_auron} class.
Every time we want to log the activation variable of the node, the activation variable can be written to the log file by the call \emph{activityVar\_logFile$<<$ [value];}
\begin{lstlisting}
activityVar_logFile <<time_class::getTid() <<"\t" 
  		    <<nAktivitetsVariabel <<";\n";
\end{lstlisting}

This writes each element a two value vector, separated by `;'. This is the syntax for matrices in octave.

The name of the log file in addition to writing the preamble required to make the log file an executable octave script is done in the auron constructor.
%  evt. skriv : of \emph{i\_auron}.  OG
%Since every derived element of a class also runs the ancestors constructor, the log will also be created for each derived class.

Every auron therefore have one log file with the path \\ \mbox{\emph{./datafiles\_for\_evaluation/log\_auron[auron name]-activityVar.oct }}

The .oct file is finalized in the aurons destructor. This includes to close the bracket for the aurons activity log--matrix, adding the commands required to making the octave script executable.
For this reason it is important to call the destructor for all the auron objects. 
Because all the auron objects are created in the free store, we will have to deallocate the memory explicitly so that the destructor for the auron is called. 
This can be done by using the \emph{delete} operator on a pointer to the obect.

Manual deletion of every auron is inconvenient and error prone for the octave script. The only thing that happens if the user forgets to free the memory is that the destructor for the auron will not be called. 
This causes the .oct script to give errors when executing the script.

For convenience, all aurons constructed will also place [this] pointer in std::list$<$i\_auron*$>$ i\_auron::pAllAurons.
The aurons destructor will remove the elment from this list.

Before the program terminates, all remaining elements in i\_aurons::pAllAurons will be deleted properly, and the destructor will be called.
This will finalize the log for each auron, and the octave scripts will be executable.

\begin{lstlisting}
  while( ! i_auron::pAllAurons.empty() ) 
  { 
    // Deallocate memory for first auron
    delete (*i_auron::pAllAurons.begin()); 
    // This will call the destrucor for
    //   the auron, which will remove it
    //   from i_auron::pAllAurons.
  }   
\end{lstlisting}

For comparison and development of synaptic transmission, each synapse also has a log file. This is most relevant for the synapses in $\kappa$ANN.
When the \emph{K\_synapse} is constructed, a log file is established for each object. This log file is created with a suitable name to separate all the log files after execution of the software.

The following code is from the constructor of \emph{K\_synapse}.
\begin{lstlisting}
std::ostringstream tempFilAdr;
tempFilAdr
  <<"./datafiles_for_evaluation/log_transmission_K_synapse_" 
  <<pPresynAuron_arg->sNavn <<"-"  <<pPostsynAuron_arg->sNavn ;
if(bInhibitoryEffect){ tempFilAdr<<"_inhi"; }
else{ 			  tempFilAdr<<"_eksi"; }
tempFilAdr<<".oct";
\end{lstlisting}

We see that this that it is possible to create two synapse--logs, for two synapses going to the same postsynaptic node, one excitatory and one inhibitory synapse.
Two synapses of the same kind could give the same result as one larger synapse(depending on the learning rule), but one excitatory and one inhibitory synapse should give an other result when synaptic plasticity is implemented.
%If future a implementation is made as a simulation of biological neural systems, we could end up differentiation between excitatory and inhibitory synapses when it comes to learning.
In this case it would be important that different log files are created.
%This could give different learning rules for excitatory and inhibitory synapses, and different log files might become important in later projects based on this code.
Se appendix \ref{appendixSecPresynapticSynapticPartOfTransmission} for a motivation for differentiating between excitatory and inhibitory synaptic plasticity and 
	for a review of synaptic plasticity in biological neural systems.
%This might be important when synaptic plasticity is implemented, as the excitatory synapses could be designed with an other set of learning rules that the inhibitory synpses.


The implementation takes arguments from shell that, for example can set the number of iterations for the next execution.
This has a default of 1000 iterations if no argument is provided.
After the arguments are read in from shell the actions are taken to keep an order in the log files.
A directory is created for the log files: The directory \emph{./datafiles\_for\_evaluation/}.
All log files are placed under this directory at the construction of the different node elements.
\begin{lstlisting}
if( system("mkdir datafiles_for_evaluation") != 0 ){
	cout<<"Could not make directory for log files "
	    <<"[./datafiles_for_evaluation/]."
		<<"Directory probably already exist."
}
//Tidy up ./datafiles_for_evaluation/
if( system("rm ./datafiles_for_evaluation/log_*.oct") != 0){
	cout<<"Could not remove old log files.\n"
	    <<"Please do this manually "
	    <<"to avoid accidendtally plotting old results.\n";
}
\end{lstlisting}
To avoid reanalysis of old results, the old log files are automaticly removed at the initiation of the execution.
This is done after the arguments have been read out from the software call.
%Before the implementation reads inn the arguments for the current execution all present \emph{.oct} log files are removed.
%This is important so that we do not analyze old results when debugging.



%\begin{figure}[htb!p]
%	\centering
%	\includegraphics[width=0.8\textwidth]{eps_auronE.eps}
%	\caption{Punktplott for spiking auron's aktivitetsVar (depol.), fra simulering. Generert ved å skrive aktivitetsVar til eit .oct fil (octave), og plotte resultatet seinere.}
%\end{figure}



